{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Tuner Guide.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f864daeea7e4b9591332896ef39693c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4aa305f34c894d71bbf7a5b25b691a5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a24f668285f24a5e822f77a59a7be47b",
              "IPY_MODEL_25fe6641eb71424a9c48b02d886dddbe"
            ]
          }
        },
        "4aa305f34c894d71bbf7a5b25b691a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a24f668285f24a5e822f77a59a7be47b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d83af341c214acdb1988c454bbfd52e",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44edda6b37e04402add6c35df76e2670"
          }
        },
        "25fe6641eb71424a9c48b02d886dddbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0733f354cc6f4ad89c5158cedb38e36a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:01&lt;00:00,  3.30 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba0fd4ee41b54b56824dbb0dcaf125de"
          }
        },
        "6d83af341c214acdb1988c454bbfd52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44edda6b37e04402add6c35df76e2670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0733f354cc6f4ad89c5158cedb38e36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba0fd4ee41b54b56824dbb0dcaf125de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "loEQF6FkhQIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "4f864daeea7e4b9591332896ef39693c",
            "4aa305f34c894d71bbf7a5b25b691a5c",
            "a24f668285f24a5e822f77a59a7be47b",
            "25fe6641eb71424a9c48b02d886dddbe",
            "6d83af341c214acdb1988c454bbfd52e",
            "44edda6b37e04402add6c35df76e2670",
            "0733f354cc6f4ad89c5158cedb38e36a",
            "ba0fd4ee41b54b56824dbb0dcaf125de"
          ]
        },
        "outputId": "acab4063-ec4c-4bd7-8348-8ee773ddb92e"
      },
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-tuner\n",
        "import kerastuner as kt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# prepare data for demonstration\n",
        "data = tfds.load('mnist')\n",
        "train_ds, test_ds = data['train'], data['test']\n",
        "\n",
        "def standardize_record(record):\n",
        "    return (tf.cast(record['image'], tf.float32) / 255.,\n",
        "            tf.one_hot(record['label'], 10))\n",
        "\n",
        "train_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\n",
        "test_ds = test_ds.map(standardize_record).cache().batch(64)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f864daeea7e4b9591332896ef39693c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiJ0q0fBpIj5",
        "colab_type": "text"
      },
      "source": [
        "A small multi-layer percepton model for the tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMty7RUdpIAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleMLP(kt.HyperModel):\n",
        "  def __init__(self,\n",
        "               x_shape,\n",
        "               y_shape,\n",
        "               max_depth=5,\n",
        "               max_units=10,\n",
        "               min_units=3,\n",
        "               min_depth=3):\n",
        "    self.max_depth = max_depth\n",
        "    self.min_depth = min_depth\n",
        "\n",
        "    self.max_units = max_units\n",
        "    self.min_units = min_units\n",
        "\n",
        "    self.x_shape = x_shape\n",
        "    self.y_shape = y_shape\n",
        "\n",
        "  def build(self, hp):\n",
        "    depth = hp.Int('depth', self.min_depth, self.max_depth)\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=self.x_shape))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    for i in range(depth):\n",
        "      units_i = hp.Int(f'units_{i}', self.min_units, self.max_units)\n",
        "      model.add(tf.keras.layers.Dense(units_i, activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(self.y_shape))\n",
        "    optimizer = hp.Choice('optimizer', ['sgd', 'adam'])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_n-23oxhRaK",
        "colab_type": "text"
      },
      "source": [
        "Keras Tuner is an easy-to-use, distributable hyperparameter optimization framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. Keras Tuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VW5_Ze5hXu8",
        "colab_type": "text"
      },
      "source": [
        "# 0. Highest level overview\n",
        "\n",
        "\n",
        "To understand how Keras Tuner works, there are three main classes that interact in the tuning loop: HyperModel, Oracle and Tuner. Tuner controls the entire workflow of repeatedly creating models, training and evaluating, and building new one based on previous results. The HyperModel is responsible for providing the pipeline to build model. The oracle is responsible for the algorithm that determines what to try in each step. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXeVzAK2O77M",
        "colab_type": "text"
      },
      "source": [
        "![level0.png](imgs/level0.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRrOPmTqO-7a",
        "colab_type": "text"
      },
      "source": [
        "For the built in oracles, each oracle have a corresponding tuner class that includes a simple `MultiExectionTuner` and the oracle itself, and hence can be initialize in one line as `tuner = kt.tuners.randomsearch.RandomSearch()` for example. \n",
        "\n",
        "Each complete cycle in the Tuner is called a `trial`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEpan9GshW-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "fb0ca1dd-a700-44dd-b0c4-1ded28d9bf65"
      },
      "source": [
        "hypermodel = SimpleMLP([28, 28,1], 10)\n",
        "oracle = kt.tuners.randomsearch.RandomSearchOracle(objective='val_accuracy', max_trials=10)\n",
        "tuner = kt.Tuner(oracle, hypermodel, overwrite=True)\n",
        "# the use of tuner.search is similar to model.fit()\n",
        "tuner.search(train_ds, validation_data=test_ds, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 18s]\n",
            "val_accuracy: 0.2451000064611435\n",
            "\n",
            "Best val_accuracy So Far: 0.6685000061988831\n",
            "Total elapsed time: 00h 03m 16s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-5uuRSBsPYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "94453d74-a848-42fb-da81-e736a129ee1a"
      },
      "source": [
        "m = tuner.get_best_models()[0]\n",
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 63        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 48        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                70        \n",
            "=================================================================\n",
            "Total params: 8,119\n",
            "Trainable params: 8,119\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5c1r5VL7Yhj",
        "colab_type": "text"
      },
      "source": [
        "# 1. hyperparameters - what hapens in each trial?\n",
        "\n",
        "In order to set hyperparameter search space or the tuning process, we need to know how Keras Tuner passes around the information about the hyperparameters.\n",
        "In Keras Tuner we have two classes, `HyperParameters` and `HyperParameter`. An object in class `HyperParameters` (hereafter `hps`) is a container of multiple objects of `HyperParameter` (hereafter `hp`). Each `hp` carries information of the allowed choices or range of a hyperparameter that we will search. Also, each `hp` is assigned with a concrete value in `hps` that is allowed by `hp`. Such `hps` objects carries all messages about the hyperparameters used for building the model. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRvtjo-jCNk7",
        "colab_type": "text"
      },
      "source": [
        "![hps2.png](imgs/hps2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CBRfLCjBjjE",
        "colab_type": "text"
      },
      "source": [
        "It is important to note that `hps` objects represent the internal message passing mechanism, and are repeatedly created and discarded along the lifetime of a `tuner`. It is always created by the `tuner` within a `trial`. After it is created, it visits `oracle` to get assigned values corresponding to each `hp` (we will discuss how `oracle` decide what to assign), and then bring the assigned values to `hypermodel` to create a new model. While building the model, the `hps` also updates itself when `hypermodel` is trying to use a `hp` that `hps` does not already contain.\n",
        "\n",
        "In the illustration below, in a `trial`, the `hps` (1) inquires value from the `oracle`, then (2) is fed into the `hypermodel` who uses the values recorded in `hps` to build a model, whereas (3) if an unseen `hp` is encountered, the `hp` is created inside `hps` with a default value, and finally (4) the information in the `hps` is recorded by `oracle` together with the `score` of this `trial`. Now it is ready for the next cycle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN3X1lTvOQ-8",
        "colab_type": "text"
      },
      "source": [
        "![KT-level1.png](imgs/KT-level1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgODyWVDD5W1",
        "colab_type": "text"
      },
      "source": [
        "Again, when using Keras-Tuner, for most of the times we are building a pipeline to pass around `hps` objects, make change to the content or retrieve value from them. Hence we usually do *not* explicitly create `hps` object. The *only* situation to manually initiate `hps` is outside of the `tuner`, when we want to pass external information into the tuning loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO5nkmeTPSM0",
        "colab_type": "text"
      },
      "source": [
        "#2 How to define search space: HyperModel\n",
        "\n",
        "Now we focus on (2) and (3), where the search space for the hyperparameter tuning is determined. \n",
        "\n",
        "An object in `HyperModel` class has a `build` function as a pipeline to build model. The signature of `build` function is always\n",
        "```python\n",
        "def build(self, hps):\n",
        "  # build the model\n",
        "  ...\n",
        "  float_0 = hps.Float('float_0', 0.1, 0.5)\n",
        "  # continue building the model while using `float_0` when needed\n",
        "  ...\n",
        "  return model\n",
        "```\n",
        "with various hyperparameters to be filled in using `hps`, where the `hps` argument will take the `hps` in each `trial`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQx1KSr5Vh5l",
        "colab_type": "text"
      },
      "source": [
        "![hypermodel.png](imgs/hypermodel.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUyNVhMVVFbv",
        "colab_type": "text"
      },
      "source": [
        " In this pipeline, if the `hps` that is passes in already contains the `hp` with name 'float_0', then `float_0` will be assigned the value provided by `hps`. If a never seen `hp` is seen, corresponding `hp` is added to the `hps` and a default value is assigned. Hence, as `hypermodel` builds the model, it is also populating the search space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fHpG-uwU3ce",
        "colab_type": "text"
      },
      "source": [
        "![hypermodel-build.png](imgs/hypermodel-build.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOTmxHSryx-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "08298b00-46f4-4b46-d80c-7d1e2b4e9930"
      },
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "class MyHyperModel(kt.HyperModel):\n",
        "  def build(self, hps):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    for i in range(4):\n",
        "      units_i = hps.Int(f'units_{i}', min_value=3, max_value=5)\n",
        "      model.add(tf.keras.layers.Dense(units_i, activation='relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "hypermodel = MyHyperModel()\n",
        "oracle = kt.tuners.randomsearch.RandomSearchOracle(objective='val_accuracy', max_trials=10)\n",
        "tuner = kt.Tuner(oracle, hypermodel, overwrite=True)\n",
        "tuner.search_space_summary()\n",
        "tuner.search(train_ds, validation_data=test_ds, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 18s]\n",
            "val_accuracy: 0.593500018119812\n",
            "\n",
            "Best val_accuracy So Far: 0.8485000133514404\n",
            "Total elapsed time: 00h 03m 11s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtCCR6RZZ92b",
        "colab_type": "text"
      },
      "source": [
        "Note: instead of creating a subclass of `HyperModel`, we can also use a function that takes `hps` and returns `model` in the place of a hypermodel. In such a case, the `tuner` will automatically create a `HyperModel` that wraps the model building function. \n",
        "\n",
        "`hypermodel` does not create `hps`, but only add new `hp` to `hps`. Besides, it never change the value of `hp` in `hps`, where new `hp` added to `hps` will assume the default value or random value by `hps`. That means within the `build` function, defining a `hp` for multiple times will get the same value on the same `trial`.\n",
        "\n",
        "`hp` is never removed from `hps` even if it is not used. This makes `oracle` easier to keep track of the history of search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrn6A3Vb0L-",
        "colab_type": "text"
      },
      "source": [
        "#3 Define search algorithms\n",
        "\n",
        "It is also possible to customize algorithms. The design of Keras Tuner completely abstracts the algorithm of tuning into `oracle`. Oracle can be thought of as a function that takes the full history of values in `hps` to determine the values for the next search. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "equg0q8-fDdd",
        "colab_type": "text"
      },
      "source": [
        "![oracle.png](imgs/oracle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK471agJfJj1",
        "colab_type": "text"
      },
      "source": [
        "The simplest search algorithm, random search, would then be described as follows:\n",
        "\n",
        "1.   Pick a random set of values that is allowed by the `hps`.\n",
        "2.   Check if the value is already tried. Go to 1 if the set of value is used. If all possible values are already tried, trigger an end for the search.\n",
        "3.   Update `hps` object with the set of value.\n",
        "\n",
        "Usually an improved algorithm will attempt to change the behavior in step 1, making use of the previous scores. For example, Bayesian optimization (also provided in Keras-Tuner) utilizes a Gaussian process regression to determine a best choice for the upcoming test.\n",
        "\n",
        "Note: `oracle` does not create `hps`, nor does it change any of the `hp` in `hps`. It only changes the values attached to it. \n",
        "\n",
        "For examples of implementing new `oracle`, see the existing ones as [examples](https://github.com/keras-team/keras-tuner/tree/master/kerastuner/tuners). Currently we provide `RandomSearch`, `BayesianOptimization` and `Hyperband`. Hyperparameter searching algorithms that consistently and significantly outperforms random search has been considered difficult to find. Hence it is usually adviced to start with `RandomSearch` if not sure what oracle to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JR9M5_7lKqo",
        "colab_type": "text"
      },
      "source": [
        "# 4 Customizing `tuner`\n",
        "\n",
        "It is also possible to define the behavior of the `tuner` by overriding `run_trial` method. A few possible occasions for needing to override `tuner` include\n",
        "1.  Need to customize training loop rather than doing simply `fit`. For example, when transfer-learning we may want to `fit` the model for two times, first time with most layers of the model frozen and the second time unfrozen. See an example [here](https://www.kaggle.com/fuyixing/flower-classification-with-keras-tuner-and-kpl)\n",
        "2.  Need to search hyperparameters that affect the behavior outside of the model building pipeline, for example if we want to tune number of epochs to train, as the example below. In this case, we need to use `trial.hyperparameters` to define new `hp`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpTEma4kqKsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyTuner(kt.Tuner):\n",
        "    def run_trial(self, trial, *fit_args, **fit_kwargs):\n",
        "        fit_kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20)\n",
        "        super(MyTuner, self).run_trial(trial, *fit_args, **fit_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm0uwzKOuaKB",
        "colab_type": "text"
      },
      "source": [
        "When customizing `tuner`, we should only override `run_trial` part which covers `hypermodel.build(hp)` to updating score of the model, and not including any direct interaction with `oracle`. Updating score can be done conveniently through adding `kt.tuner_utils.TunerCallback` to `fit`, or manually update using the [hooks](https://keras-team.github.io/keras-tuner/tutorials/subclass-tuner/#overriding-run_trial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iO7hfsHikb1",
        "colab_type": "text"
      },
      "source": [
        "# Summary of hps, hypermodel, oracle and tuner\n",
        "\n",
        "|   | hypermodel  | oracle  | tuner  |\n",
        "|---|---|---|---|\n",
        "| create `hps`  | no  | no  | yes  |\n",
        "| add `hp` to `hps`  | when first encountered  | no  | maybe, within `run_trial`  |\n",
        "| remove `hp` from `hps`  | no  | no  | no  |\n",
        "| change value of `hp` in `hps` | no | yes | no |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1wERtkLZb82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
