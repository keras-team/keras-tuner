<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Documentation for Keras Tuner.">
        
        
        <link rel="shortcut icon" href="img/favicon.ico">
        <title>Keras Tuner</title>
        <link href="css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="css/font-awesome.min.css" rel="stylesheet">
        <link href="css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="js/jquery-1.10.2.min.js" defer></script>
        <script src="js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-61785484-2', 'keras-team.github.io/keras-tuner');
            ga('send', 'pageview');
        </script> 
    </head>

    <body class="homepage">

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href=".">Keras Tuner</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="active">
                                <a href=".">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorials <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="tutorials/distributed-tuning/">Distributed Tuning</a>
</li>
                                    
<li >
    <a href="tutorials/subclass-tuner/">Subclassing Tuner for Custom Training Loops</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="documentation/hypermodels/">HyperModels</a>
</li>
                                    
<li >
    <a href="documentation/hyperparameters/">HyperParameters</a>
</li>
                                    
<li >
    <a href="documentation/oracles/">Oracles</a>
</li>
                                    
<li >
    <a href="documentation/tuners/">Tuners</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="examples/helloworld/">Hello World</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="contributing/">Contributing Guide</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="disabled">
                                <a rel="next" >
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="tutorials/distributed-tuning/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/keras-team/keras-tuner"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#keras-tuner-documentation">Keras Tuner documentation</a></li>
            <li><a href="#installation">Installation</a></li>
            <li><a href="#usage-the-basics">Usage: the basics</a></li>
            <li><a href="#the-search-space-may-contain-conditional-hyperparameters">The search space may contain conditional hyperparameters</a></li>
            <li><a href="#you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function">You can use a HyperModel subclass instead of a model-building function</a></li>
            <li><a href="#keras-tuner-includes-pre-made-tunable-applications-hyperresnet-and-hyperxception">Keras Tuner includes pre-made tunable applications: HyperResNet and HyperXception</a></li>
            <li><a href="#you-can-easily-restrict-the-search-space-to-just-a-few-parameters">You can easily restrict the search space to just a few parameters</a></li>
            <li><a href="#about-parameter-default-values">About parameter default values</a></li>
            <li><a href="#fixing-values-in-a-hypermodel">Fixing values in a hypermodel</a></li>
            <li><a href="#overriding-compilation-arguments">Overriding compilation arguments</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="keras-tuner-documentation">Keras Tuner documentation</h1>
<h2 id="installation">Installation</h2>
<p>Requirements:</p>
<ul>
<li>Python 3.6</li>
<li>TensorFlow 2.0</li>
</ul>
<p>Install latest release:</p>
<pre><code>pip install -U keras-tuner
</code></pre>

<p>Install from source:</p>
<pre><code>git clone https://github.com/keras-team/keras-tuner.git
cd keras-tuner
pip install .
</code></pre>

<h2 id="usage-the-basics">Usage: the basics</h2>
<p>Here's how to perform hyperparameter tuning for a single-layer dense neural network using random search.</p>
<p>First, we define a model-building function. It takes an argument <code>hp</code> from which you can
sample hyperparameters, such as <code>hp.Int('units', min_value=32, max_value=512, step=32)</code>
(an integer from a certain range).</p>
<p>This function returns a compiled model.</p>
<pre><code class="python">from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch


def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Dense(units=hp.Int('units',
                                        min_value=32,
                                        max_value=512,
                                        step=32),
                           activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate',
                      values=[1e-2, 1e-3, 1e-4])),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])
    return model
</code></pre>

<p>Next, instantiate a tuner. You should specify the model-building function,
the name of the objective to optimize (whether to minimize or maximize is automatically inferred
for built-in metrics), the total number of trials (<code>max_trials</code>) to test, and the number
of models that should be built and fit for each trial (<code>executions_per_trial</code>).</p>
<p>Available tuners are <code>RandomSearch</code> and <code>Hyperband</code>.</p>
<p><strong>Note:</strong> the purpose of having multiple executions per trial is to reduce results variance
and therefore be able to more accurately assess the performance of a model. If you want to get
results faster, you could set <code>executions_per_trial=1</code> (single round of training for each model configuration).</p>
<pre><code class="python">tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='my_dir',
    project_name='helloworld')
</code></pre>

<p>You can print a summary of the search space:</p>
<pre><code class="python">tuner.search_space_summary()
</code></pre>

<p>Then, start the search for the best hyperparameter configuration.
The call to <code>search</code> has the same signature as <code>model.fit()</code>.</p>
<pre><code class="python">tuner.search(x, y,
             epochs=5,
             validation_data=(val_x, val_y))
</code></pre>

<p>Here's what happens in <code>search</code>: models are built iteratively by calling the model-building function,
which populates the hyperparameter space (search space) tracked by the <code>hp</code> object. The tuner
progressively explores the space, recording metrics for each configuration.</p>
<p>When search is over, you can retrieve the best model(s):</p>
<pre><code class="python">models = tuner.get_best_models(num_models=2)
</code></pre>

<p>Or print a summary of the results:</p>
<pre><code class="python">tuner.results_summary()
</code></pre>

<p>You will also find detailed logs, checkpoints, etc, in the folder <code>my_dir/helloworld</code>, i.e. <code>directory/project_name</code>.</p>
<h2 id="the-search-space-may-contain-conditional-hyperparameters">The search space may contain conditional hyperparameters</h2>
<p>Below, we have a <code>for</code> loop creating a tunable number of layers,
which themselves involve a tunable <code>units</code> parameter.</p>
<p>This can be pushed to any level of parameter interdependency, including recursion.</p>
<p>Note that all parameter names should be unique (here, in the loop over <code>i</code>,
we name the inner parameters <code>'units_' + str(i)</code>).</p>
<pre><code class="python">def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])
    return model
</code></pre>

<h2 id="you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function">You can use a HyperModel subclass instead of a model-building function</h2>
<p>This makes it easy to share and reuse hypermodels.</p>
<p>A <code>HyperModel</code> subclass only needs to implement a <code>build(self, hp)</code> method.</p>
<pre><code class="python">from kerastuner import HyperModel


class MyHyperModel(HyperModel):

    def __init__(self, num_classes):
        self.num_classes = num_classes

    def build(self, hp):
        model = keras.Sequential()
        model.add(layers.Dense(units=hp.Int('units',
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
        model.add(layers.Dense(self.num_classes, activation='softmax'))
        model.compile(
            optimizer=keras.optimizers.Adam(
                hp.Choice('learning_rate',
                          values=[1e-2, 1e-3, 1e-4])),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])
        return model


hypermodel = MyHyperModel(num_classes=10)

tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    directory='my_dir',
    project_name='helloworld')

tuner.search(x, y,
             epochs=5,
             validation_data=(val_x, val_y))
</code></pre>

<h2 id="keras-tuner-includes-pre-made-tunable-applications-hyperresnet-and-hyperxception">Keras Tuner includes pre-made tunable applications: HyperResNet and HyperXception</h2>
<p>These are ready-to-use hypermodels for computer vision.</p>
<p>They come pre-compiled with <code>loss="categorical_crossentropy"</code> and <code>metrics=["accuracy"]</code>.</p>
<pre><code class="python">from kerastuner.applications import HyperResNet
from kerastuner.tuners import Hyperband

hypermodel = HyperResNet(input_shape=(128, 128, 3), num_classes=10)

tuner = Hyperband(
    hypermodel,
    objective='val_accuracy',
    max_trials=40,
    directory='my_dir',
    project_name='helloworld')

tuner.search(x, y,
             epochs=20,
             validation_data=(val_x, val_y))
</code></pre>

<h2 id="you-can-easily-restrict-the-search-space-to-just-a-few-parameters">You can easily restrict the search space to just a few parameters</h2>
<p>If you have an existing hypermodel, and you want to search over only a few parameters
(such as the learning rate), you can do so by passing a <code>hyperparameters</code> argument
to the tuner constructor, as well as <code>tune_new_entries=False</code> to specify that parameters
that you didn't list in <code>hyperparameters</code> should not be tuned. For these parameters, the default
value gets used.</p>
<pre><code class="python">from kerastuner import HyperParameters

hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10)

hp = HyperParameters()
# This will override the `learning_rate` parameter with your
# own selection of choices
hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

tuner = Hyperband(
    hypermodel,
    hyperparameters=hp,
    # `tune_new_entries=False` prevents unlisted parameters from being tuned
    tune_new_entries=False,
    objective='val_accuracy',
    max_trials=40,
    directory='my_dir',
    project_name='helloworld')

tuner.search(x, y,
             epochs=20,
             validation_data=(val_x, val_y))
</code></pre>

<p>Want to know what parameter names are available? Read the code.</p>
<h2 id="about-parameter-default-values">About parameter default values</h2>
<p>Whenever you register a hyperparameter inside a model-building function or the <code>build</code> method of a hypermodel,
you can specify a default value:</p>
<pre><code class="python">hp.Int('units',
       min_value=32,
       max_value=512,
       step=32,
       default=128)
</code></pre>

<p>If you don't, hyperparameters always have a default default (for <code>Int</code>, it is equal to <code>min_value</code>).</p>
<h2 id="fixing-values-in-a-hypermodel">Fixing values in a hypermodel</h2>
<p>What if you want to do the reverse -- tune all available parameters in a hypermodel, <strong>except</strong> one (the learning rate)?</p>
<p>Pass a <code>hyperparameters</code> argument with a <code>Fixed</code> entry (or any number of <code>Fixed</code> entries), and specify <code>tune_new_entries=True</code>.</p>
<pre><code class="python">hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10)

hp = HyperParameters()
hp.Fixed('learning_rate', value=1e-4)

tuner = Hyperband(
    hypermodel,
    hyperparameters=hp,
    tune_new_entries=True,
    objective='val_accuracy',
    max_trials=40,
    directory='my_dir',
    project_name='helloworld')

tuner.search(x, y,
             epochs=20,
             validation_data=(val_x, val_y))
</code></pre>

<h2 id="overriding-compilation-arguments">Overriding compilation arguments</h2>
<p>If you have a hypermodel for which you want to change the existing optimizer, loss, or metrics, you can do so by passing these arguments
to the tuner constructor:</p>
<pre><code class="python">hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10)

tuner = Hyperband(
    hypermodel,
    optimizer=keras.optimizers.Adam(1e-3),
    loss='mse',
    metrics=[keras.metrics.Precision(name='precision'),
             keras.metrics.Recall(name='recall')],
    objective='val_precision',
    max_trials=40,
    directory='my_dir',
    project_name='helloworld')

tuner.search(x, y,
             epochs=20,
             validation_data=(val_x, val_y))
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = ".",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="js/base.js" defer></script>
        <script src="search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-11-01 21:15:20
-->
