<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Tuners - Keras Tuner</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../../extra.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">Keras Tuner</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorials <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../tutorials/distributed-tuning/">Distributed Tuning</a>
</li>
                                    
<li >
    <a href="../../tutorials/subclass-tuner/">Subclassing Tuner for Custom Training Loops</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../hypermodels/">HyperModels</a>
</li>
                                    
<li >
    <a href="../hyperparameters/">HyperParameters</a>
</li>
                                    
<li >
    <a href="../oracles/">Oracles</a>
</li>
                                    
<li class="active">
    <a href="./">Tuners</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../examples/helloworld/">Hello World</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../contributing/">Contributing Guide</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../oracles/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../../examples/helloworld/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/keras-team/keras-tuner"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#tuners">Tuners</a></li>
            <li><a href="#bayesianoptimization-class">BayesianOptimization class:</a></li>
            <li><a href="#hyperband-class">Hyperband class:</a></li>
            <li><a href="#randomsearch-class">RandomSearch class:</a></li>
            <li><a href="#sklearn-class">Sklearn class:</a></li>
            <li><a href="#tuner-class">Tuner class:</a></li>
            <li><a href="#get_best_models-method">get_best_models method:</a></li>
            <li><a href="#get_state-method">get_state method:</a></li>
            <li><a href="#load_model-method">load_model method:</a></li>
            <li><a href="#on_epoch_begin-method">on_epoch_begin method:</a></li>
            <li><a href="#on_batch_begin-method">on_batch_begin method:</a></li>
            <li><a href="#on_batch_end-method">on_batch_end method:</a></li>
            <li><a href="#on_epoch_end-method">on_epoch_end method:</a></li>
            <li><a href="#run_trial-method">run_trial method:</a></li>
            <li><a href="#save_model-method">save_model method:</a></li>
            <li><a href="#search-method">search method:</a></li>
            <li><a href="#set_state-method">set_state method:</a></li>
            <li><a href="#basetuner-class">BaseTuner class:</a></li>
            <li><a href="#get_best_hyperparameters-method">get_best_hyperparameters method:</a></li>
            <li><a href="#get_best_models-method_1">get_best_models method:</a></li>
            <li><a href="#get_state-method_1">get_state method:</a></li>
            <li><a href="#load_model-method_1">load_model method:</a></li>
            <li><a href="#run_trial-method_1">run_trial method:</a></li>
            <li><a href="#save_model-method_1">save_model method:</a></li>
            <li><a href="#search-method_1">search method:</a></li>
            <li><a href="#set_state-method_1">set_state method:</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="tuners">Tuners</h1>
<p>Tuners are here to do the hyperparameter search. You can create custom Tuners by subclassing <code>kerastuner.engine.tuner.Tuner</code>.</p>
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/tuners/bayesian.py#L270">[source]</a></span></p>
<h3 id="bayesianoptimization-class">BayesianOptimization class:</h3>
<pre><code class="python">kerastuner.tuners.bayesian.BayesianOptimization(hypermodel, objective, max_trials, num_initial_points=2, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)
</code></pre>

<p>BayesianOptimization tuning with Gaussian process.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>hypermodel</strong>: Instance of HyperModel class
    (or callable that takes hyperparameters
    and returns a Model instance).</li>
<li><strong>objective</strong>: String. Name of model metric to minimize
    or maximize, e.g. "val_accuracy".</li>
<li><strong>max_trials</strong>: Int. Total number of trials
    (model configurations) to test at most.
    Note that the oracle may interrupt the search
    before <code>max_trial</code> models have been tested if the search space has
    been exhausted.</li>
<li><strong>num_initial_points</strong>: Int. The number of randomly generated samples as initial
    training data for Bayesian optimization.</li>
<li><strong>alpha</strong>: Float or array-like. Value added to the diagonal of
    the kernel matrix during fitting.</li>
<li><strong>beta</strong>: Float. The balancing factor of exploration and exploitation.
    The larger it is, the more explorative it is.</li>
<li><strong>seed</strong>: Int. Random seed.</li>
<li><strong>hyperparameters</strong>: HyperParameters class instance.
    Can be used to override (or register in advance)
    hyperparamters in the search space.</li>
<li><strong>tune_new_entries</strong>: Whether hyperparameter entries
    that are requested by the hypermodel
    but that were not specified in <code>hyperparameters</code>
    should be added to the search space, or not.
    If not, then the default value for these parameters
    will be used.</li>
<li><strong>allow_new_entries</strong>: Whether the hypermodel is allowed
    to request hyperparameter entries not listed in
    <code>hyperparameters</code>.</li>
<li><strong>**kwargs</strong>: Keyword arguments relevant to all <code>Tuner</code> subclasses.
    Please see the docstring for <code>Tuner</code>.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/tuners/hyperband.py#L312">[source]</a></span></p>
<h3 id="hyperband-class">Hyperband class:</h3>
<pre><code class="python">kerastuner.tuners.hyperband.Hyperband(hypermodel, objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)
</code></pre>

<p>Variation of HyperBand algorithm.</p>
<p>Reference:
Li, Lisha, and Kevin Jamieson.
<a href="http://jmlr.org/papers/v18/16-558.html">"Hyperband: A Novel Bandit-Based
Approach to Hyperparameter Optimization."
Journal of Machine Learning Research 18 (2018): 1-52</a>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>hypermodel</strong>: Instance of HyperModel class
    (or callable that takes hyperparameters
    and returns a Model instance).</li>
<li><strong>objective</strong>: String. Name of model metric to minimize
    or maximize, e.g. "val_accuracy".</li>
<li><strong>max_epochs</strong>: Int. The maximum number of epochs to train one model. It is
  recommended to set this to a value slightly higher than the expected time
  to convergence for your largest Model, and to use early stopping during
  training (for example, via <code>tf.keras.callbacks.EarlyStopping</code>).</li>
<li><strong>factor</strong>: Int. Reduction factor for the number of epochs
    and number of models for each bracket.</li>
<li><strong>hyperband_iterations</strong>: Int &gt;= 1. The number of times to iterate over the full
  Hyperband algorithm. One iteration will run approximately
  <code>max_epochs * (math.log(max_epochs, factor) ** 2)</code> cumulative epochs
  across all trials. It is recommended to set this to as high a value
  as is within your resource budget.</li>
<li><strong>seed</strong>: Int. Random seed.</li>
<li><strong>hyperparameters</strong>: HyperParameters class instance.
    Can be used to override (or register in advance)
    hyperparamters in the search space.</li>
<li><strong>tune_new_entries</strong>: Whether hyperparameter entries
    that are requested by the hypermodel
    but that were not specified in <code>hyperparameters</code>
    should be added to the search space, or not.
    If not, then the default value for these parameters
    will be used.</li>
<li><strong>allow_new_entries</strong>: Whether the hypermodel is allowed
    to request hyperparameter entries not listed in
    <code>hyperparameters</code>.</li>
<li><strong>**kwargs</strong>: Keyword arguments relevant to all <code>Tuner</code> subclasses.
    Please see the docstring for <code>Tuner</code>.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/tuners/randomsearch.py#L125">[source]</a></span></p>
<h3 id="randomsearch-class">RandomSearch class:</h3>
<pre><code class="python">kerastuner.tuners.randomsearch.RandomSearch(hypermodel, objective, max_trials, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)
</code></pre>

<p>Random search tuner.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>hypermodel</strong>: Instance of HyperModel class
    (or callable that takes hyperparameters
    and returns a Model instance).</li>
<li><strong>objective</strong>: String. Name of model metric to minimize
    or maximize, e.g. "val_accuracy".</li>
<li><strong>max_trials</strong>: Int. Total number of trials
    (model configurations) to test at most.
    Note that the oracle may interrupt the search
    before <code>max_trial</code> models have been tested.</li>
<li><strong>seed</strong>: Int. Random seed.</li>
<li><strong>hyperparameters</strong>: HyperParameters class instance.
    Can be used to override (or register in advance)
    hyperparamters in the search space.</li>
<li><strong>tune_new_entries</strong>: Whether hyperparameter entries
    that are requested by the hypermodel
    but that were not specified in <code>hyperparameters</code>
    should be added to the search space, or not.
    If not, then the default value for these parameters
    will be used.</li>
<li><strong>allow_new_entries</strong>: Whether the hypermodel is allowed
    to request hyperparameter entries not listed in
    <code>hyperparameters</code>.</li>
<li><strong>**kwargs</strong>: Keyword arguments relevant to all <code>Tuner</code> subclasses.
    Please see the docstring for <code>Tuner</code>.</li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/tuners/sklearn.py#L25">[source]</a></span></p>
<h3 id="sklearn-class">Sklearn class:</h3>
<pre><code class="python">kerastuner.tuners.sklearn.Sklearn(oracle, hypermodel, scoring=None, metrics=None, cv=KFold(n_splits=5, random_state=1, shuffle=True), **kwargs)
</code></pre>

<p>Tuner for Scikit-learn Models.</p>
<p>Performs cross-validated hyperparameter search for Scikit-learn
models.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>oracle</strong>: An instance of the <code>kerastuner.Oracle</code> class. Note that for
  this <code>Tuner</code>, the <code>objective</code> for the <code>Oracle</code> should always be set
  to <code>Objective('score', direction='max')</code>. Also, <code>Oracle</code>s that exploit
  Neural-Network-specific training (e.g. <code>Hyperband</code>) should not be
  used with this <code>Tuner</code>.</li>
<li><strong>hypermodel</strong>: Instance of <code>HyperModel</code> class (or callable that takes a
  <code>Hyperparameters</code> object and returns a Model instance).</li>
<li><strong>scoring</strong>: An sklearn <code>scoring</code> function. For more information, see
  <code>sklearn.metrics.make_scorer</code>. If not provided, the Model's default
  scoring will be used via <code>model.score</code>. Note that if you are searching
  across different Model families, the default scoring for these Models
  will often be different. In this case you should supply <code>scoring</code> here
  in order to make sure your Models are being scored on the same metric.</li>
<li><strong>metrics</strong>: Additional <code>sklearn.metrics</code> functions to monitor during search.
  Note that these metrics do not affect the search process.</li>
<li><strong>cv</strong>: An <code>sklearn.model_selection</code> Splitter class. Used to
  determine how samples are split up into groups for cross-validation.</li>
<li><strong>**kwargs</strong>: Keyword arguments relevant to all <code>Tuner</code> subclasses. Please
  see the docstring for <code>Tuner</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="python">import kerastuner as kt
from sklearn import ensemble
from sklearn import datasets
from sklearn import linear_model
from sklearn import metrics
from sklearn import model_selection

def build_model(hp):
  model_type = hp.Choice('model_type', ['random_forest', 'ridge'])
  if model_type == 'random_forest':
    model = ensemble.RandomForestClassifier(
        n_estimators=hp.Int('n_estimators', 10, 50, step=10),
        max_depth=hp.Int('max_depth', 3, 10))
  else:
    model = linear_model.RidgeClassifier(
        alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))
  return model

tuner = kt.tuners.Sklearn(
    oracle=kt.oracles.BayesianOptimization(
        objective=kt.Objective('score', 'max'),
        max_trials=10),
    hypermodel=build_model,
    scoring=metrics.make_scorer(metrics.accuracy_score),
    cv=model_selection.StratifiedKFold(5),
    directory='.',
    project_name='my_project')

X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = model_selection.train_test_split(
    X, y, test_size=0.2)

tuner.search(X_train, y_train)

best_model = tuner.get_best_models(num_models=1)[0]
</code></pre>

<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/engine/tuner.py#L30">[source]</a></span></p>
<h3 id="tuner-class">Tuner class:</h3>
<pre><code class="python">kerastuner.engine.tuner.Tuner(oracle, hypermodel, max_model_size=None, optimizer=None, loss=None, metrics=None, distribution_strategy=None, directory=None, project_name=None, logger=None, tuner_id=None, overwrite=False)
</code></pre>

<p>Tuner class for Keras models.</p>
<p>May be subclassed to create new tuners.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>oracle</strong>: Instance of Oracle class.</li>
<li><strong>hypermodel</strong>: Instance of HyperModel class
    (or callable that takes hyperparameters
    and returns a Model instance).</li>
<li><strong>max_model_size</strong>: Int. Maximum size of weights
    (in floating point coefficients) for a valid
    models. Models larger than this are rejected.</li>
<li><strong>optimizer</strong>: Optional. Optimizer instance.
    May be used to override the <code>optimizer</code>
    argument in the <code>compile</code> step for the
    models. If the hypermodel
    does not compile the models it generates,
    then this argument must be specified.</li>
<li><strong>loss</strong>: Optional. May be used to override the <code>loss</code>
    argument in the <code>compile</code> step for the
    models. If the hypermodel
    does not compile the models it generates,
    then this argument must be specified.</li>
<li><strong>metrics</strong>: Optional. May be used to override the
    <code>metrics</code> argument in the <code>compile</code> step
    for the models. If the hypermodel
    does not compile the models it generates,
    then this argument must be specified.</li>
<li><strong>distribution_strategy</strong>: Optional. A TensorFlow
    <code>tf.distribute</code> DistributionStrategy instance. If
    specified, each trial will run under this scope. For
    example, <code>tf.distribute.MirroredStrategy(['/gpu:0, /'gpu:1])</code>
    will run each trial on two GPUs. Currently only
    single-worker strategies are supported.</li>
<li><strong>directory</strong>: String. Path to the working directory (relative).</li>
<li><strong>project_name</strong>: Name to use as prefix for files saved
    by this Tuner.</li>
<li><strong>logger</strong>: Optional. Instance of Logger class, used for streaming data
    to Cloud Service for monitoring.</li>
<li><strong>overwrite</strong>: Bool, default <code>False</code>. If <code>False</code>, reloads an existing project
    of the same name if one is found. Otherwise, overwrites the project.</li>
</ul>
<hr />
<h3 id="get_best_models-method">get_best_models method:</h3>
<pre><code class="python">Tuner.get_best_models(num_models=1)
</code></pre>

<p>Returns the best model(s), as determined by the tuner's objective.</p>
<p>The models are loaded with the weights corresponding to
their best checkpoint (at the end of the best epoch of best trial).</p>
<p>This method is only a convenience shortcut. For best performance, It is
recommended to retrain your Model on the full dataset using the best
hyperparameters found during <code>search</code>.</p>
<p>Args:
num_models (int, optional): Number of best models to return.
Models will be returned in sorted order. Defaults to 1.</p>
<p>Returns:
List of trained model instances.</p>
<hr />
<h3 id="get_state-method">get_state method:</h3>
<pre><code class="python">BaseTuner.get_state()
</code></pre>

<p>Returns the current state of this object.</p>
<p>This method is called during <code>save</code>.</p>
<hr />
<h3 id="load_model-method">load_model method:</h3>
<pre><code class="python">Tuner.load_model(trial)
</code></pre>

<p>Loads a Model from a given trial.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance. For models that report intermediate
  results to the <code>Oracle</code>, generally <code>load_model</code> should load the
  best reported <code>step</code> by relying of <code>trial.best_step</code></li>
</ul>
<hr />
<h3 id="on_epoch_begin-method">on_epoch_begin method:</h3>
<pre><code class="python">Tuner.on_epoch_begin(trial, model, epoch, logs=None)
</code></pre>

<p>A hook called at the start of every epoch.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance.</li>
<li><strong>model</strong>: A Keras <code>Model</code>.</li>
<li><strong>epoch</strong>: The current epoch number.</li>
<li><strong>logs</strong>: Additional metrics.</li>
</ul>
<hr />
<h3 id="on_batch_begin-method">on_batch_begin method:</h3>
<pre><code class="python">Tuner.on_batch_begin(trial, model, batch, logs)
</code></pre>

<p>A hook called at the start of every batch.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance.</li>
<li><strong>model</strong>: A Keras <code>Model</code>.</li>
<li><strong>batch</strong>: The current batch number within the
  curent epoch.</li>
<li><strong>logs</strong>: Additional metrics.</li>
</ul>
<hr />
<h3 id="on_batch_end-method">on_batch_end method:</h3>
<pre><code class="python">Tuner.on_batch_end(trial, model, batch, logs=None)
</code></pre>

<p>A hook called at the end of every batch.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance.</li>
<li><strong>model</strong>: A Keras <code>Model</code>.</li>
<li><strong>batch</strong>: The current batch number within the
  curent epoch.</li>
<li><strong>logs</strong>: Additional metrics.</li>
</ul>
<hr />
<h3 id="on_epoch_end-method">on_epoch_end method:</h3>
<pre><code class="python">Tuner.on_epoch_end(trial, model, epoch, logs=None)
</code></pre>

<p>A hook called at the end of every epoch.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance.</li>
<li><strong>model</strong>: A Keras <code>Model</code>.</li>
<li><strong>epoch</strong>: The current epoch number.</li>
<li><strong>logs</strong>: Dict. Metrics for this epoch. This should include
  the value of the objective for this epoch.</li>
</ul>
<hr />
<h3 id="run_trial-method">run_trial method:</h3>
<pre><code class="python">Tuner.run_trial(trial, *fit_args, **fit_kwargs)
</code></pre>

<p>Evaluates a set of hyperparameter values.</p>
<p>This method is called during <code>search</code> to evaluate a set of
hyperparameters.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance that contains the information
  needed to run this trial. <code>Hyperparameters</code> can be accessed
  via <code>trial.hyperparameters</code>.</li>
<li><strong>*fit_args</strong>: Positional arguments passed by <code>search</code>.</li>
<li><strong>*fit_kwargs</strong>: Keyword arguments passed by <code>search</code>.</li>
</ul>
<hr />
<h3 id="save_model-method">save_model method:</h3>
<pre><code class="python">Tuner.save_model(trial_id, model, step=0)
</code></pre>

<p>Saves a Model for a given trial.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial_id</strong>: The ID of the <code>Trial</code> that corresponds to this Model.</li>
<li><strong>model</strong>: The trained model.</li>
<li><strong>step</strong>: For models that report intermediate results to the <code>Oracle</code>,
  the step that this saved file should correspond to. For example,
  for Keras models this is the number of epochs trained.</li>
</ul>
<hr />
<h3 id="search-method">search method:</h3>
<pre><code class="python">BaseTuner.search(*fit_args, **fit_kwargs)
</code></pre>

<p>Performs a search for best hyperparameter configuations.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>*fit_args</strong>: Positional arguments that should be passed to
  <code>run_trial</code>, for example the training and validation data.</li>
<li><strong>*fit_kwargs</strong>: Keyword arguments that should be passed to
  <code>run_trial</code>, for example the training and validation data.</li>
</ul>
<hr />
<h3 id="set_state-method">set_state method:</h3>
<pre><code class="python">BaseTuner.set_state(state)
</code></pre>

<p>Sets the current state of this object.</p>
<p>This method is called during <code>reload</code>.</p>
<p><strong>Arguments:</strong></p>
<p>state: Dict. The state to restore for this object.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras-tuner/blob/master/kerastuner/engine/base_tuner.py#L35">[source]</a></span></p>
<h3 id="basetuner-class">BaseTuner class:</h3>
<pre><code class="python">kerastuner.engine.base_tuner.BaseTuner(oracle, hypermodel, directory=None, project_name=None, logger=None, overwrite=False)
</code></pre>

<p>Tuner base class.</p>
<p>May be subclassed to create new tuners, including for non-Keras models.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>oracle</strong>: Instance of Oracle class.</li>
<li><strong>hypermodel</strong>: Instance of HyperModel class
    (or callable that takes hyperparameters
    and returns a Model instance).</li>
<li><strong>directory</strong>: String. Path to the working directory (relative).</li>
<li><strong>project_name</strong>: Name to use as prefix for files saved
    by this Tuner.</li>
<li><strong>logger</strong>: Optional. Instance of Logger class, used for streaming data
    to Cloud Service for monitoring.</li>
<li><strong>overwrite</strong>: Bool, default <code>False</code>. If <code>False</code>, reloads an existing project
    of the same name if one is found. Otherwise, overwrites the project.</li>
</ul>
<hr />
<h3 id="get_best_hyperparameters-method">get_best_hyperparameters method:</h3>
<pre><code class="python">BaseTuner.get_best_hyperparameters(num_trials=1)
</code></pre>

<p>Returns the best hyperparameters, as determined by the objective.</p>
<p>This method can be used to reinstantiate the (untrained) best model
found during the search process.</p>
<p>Example:</p>
<pre><code class="python">best_hp = tuner.get_best_hyperparameters()[0]
model = tuner.hypermodel.build(best_hp)
</code></pre>

<p><strong>Arguments:</strong></p>
<ul>
<li><strong>num_trials</strong>: (int, optional). Number of <code>HyperParameters</code> objects to
  return. <code>HyperParameters</code> will be returned in sorted order based on
  trial performance.</li>
</ul>
<p><strong>Returns:</strong></p>
<p>List of <code>HyperParameter</code> objects.</p>
<hr />
<h3 id="get_best_models-method_1">get_best_models method:</h3>
<pre><code class="python">BaseTuner.get_best_models(num_models=1)
</code></pre>

<p>Returns the best model(s), as determined by the objective.</p>
<p>This method is only a convenience shortcut. For best performance, It is
recommended to retrain your Model on the full dataset using the best
hyperparameters found during <code>search</code>.</p>
<p><strong>Arguments:</strong></p>
<p>num_models (int, optional). Number of best models to return.
    Models will be returned in sorted order. Defaults to 1.</p>
<p><strong>Returns:</strong></p>
<p>List of trained model instances.</p>
<hr />
<h3 id="get_state-method_1">get_state method:</h3>
<pre><code class="python">BaseTuner.get_state()
</code></pre>

<p>Returns the current state of this object.</p>
<p>This method is called during <code>save</code>.</p>
<hr />
<h3 id="load_model-method_1">load_model method:</h3>
<pre><code class="python">BaseTuner.load_model(trial)
</code></pre>

<p>Loads a Model from a given trial.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance. For models that report intermediate
  results to the <code>Oracle</code>, generally <code>load_model</code> should load the
  best reported <code>step</code> by relying of <code>trial.best_step</code></li>
</ul>
<hr />
<h3 id="run_trial-method_1">run_trial method:</h3>
<pre><code class="python">BaseTuner.run_trial(trial, *fit_args, **fit_kwargs)
</code></pre>

<p>Evaluates a set of hyperparameter values.</p>
<p>This method is called during <code>search</code> to evaluate a set of
hyperparameters.</p>
<p>For subclass implementers: This method is responsible for
reporting metrics related to the <code>Trial</code> to the <code>Oracle</code>
via <code>self.oracle.update_trial</code>.</p>
<p>Simplest example:</p>
<pre><code class="python">def run_trial(self, trial, x, y, val_x, val_y):
    model = self.hypermodel.build(trial.hyperparameters)
    model.fit(x, y)
    loss = model.evaluate(val_x, val_y)
    self.oracle.update_trial(
      trial.trial_id, {'loss': loss})
    self.save_model(trial.trial_id, model)
</code></pre>

<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial</strong>: A <code>Trial</code> instance that contains the information
  needed to run this trial. Hyperparameters can be accessed
  via <code>trial.hyperparameters</code>.</li>
<li><strong>*fit_args</strong>: Positional arguments passed by <code>search</code>.</li>
<li><strong>*fit_kwargs</strong>: Keyword arguments passed by <code>search</code>.</li>
</ul>
<hr />
<h3 id="save_model-method_1">save_model method:</h3>
<pre><code class="python">BaseTuner.save_model(trial_id, model, step=0)
</code></pre>

<p>Saves a Model for a given trial.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>trial_id</strong>: The ID of the <code>Trial</code> that corresponds to this Model.</li>
<li><strong>model</strong>: The trained model.</li>
<li><strong>step</strong>: For models that report intermediate results to the <code>Oracle</code>,
  the step that this saved file should correspond to. For example,
  for Keras models this is the number of epochs trained.</li>
</ul>
<hr />
<h3 id="search-method_1">search method:</h3>
<pre><code class="python">BaseTuner.search(*fit_args, **fit_kwargs)
</code></pre>

<p>Performs a search for best hyperparameter configuations.</p>
<p><strong>Arguments:</strong></p>
<ul>
<li><strong>*fit_args</strong>: Positional arguments that should be passed to
  <code>run_trial</code>, for example the training and validation data.</li>
<li><strong>*fit_kwargs</strong>: Keyword arguments that should be passed to
  <code>run_trial</code>, for example the training and validation data.</li>
</ul>
<hr />
<h3 id="set_state-method_1">set_state method:</h3>
<pre><code class="python">BaseTuner.set_state(state)
</code></pre>

<p>Sets the current state of this object.</p>
<p>This method is called during <code>reload</code>.</p>
<p><strong>Arguments:</strong></p>
<p>state: Dict. The state to restore for this object.</p>
<hr /></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
