{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Keras Tuner documentation Installation Requirements: Python 3.6 TensorFlow 2.0 Install latest release: pip install -U keras-tuner Install from source: git clone https://github.com/keras-team/keras-tuner.git cd keras-tuner pip install . Usage: the basics Here's how to perform hyperparameter tuning for a single-layer dense neural network using random search. First, we define a model-building function. It takes an argument hp from which you can sample hyperparameters, such as hp.Int('units', min_value=32, max_value=512, step=32) (an integer from a certain range). This function returns a compiled model. from tensorflow import keras from tensorflow.keras import layers from kerastuner.tuners import RandomSearch def build_model(hp): model = keras.Sequential() model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model Next, instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials ( max_trials ) to test, and the number of models that should be built and fit for each trial ( executions_per_trial ). Available tuners are RandomSearch and Hyperband . Note: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration). tuner = RandomSearch( build_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='my_dir', project_name='helloworld') You can print a summary of the search space: tuner.search_space_summary() Then, start the search for the best hyperparameter configuration. The call to search has the same signature as model.fit() . tuner.search(x, y, epochs=5, validation_data=(val_x, val_y)) Here's what happens in search : models are built iteratively by calling the model-building function, which populates the hyperparameter space (search space) tracked by the hp object. The tuner progressively explores the space, recording metrics for each configuration. When search is over, you can retrieve the best model(s): models = tuner.get_best_models(num_models=2) Or print a summary of the results: tuner.results_summary() You will also find detailed logs, checkpoints, etc, in the folder my_dir/helloworld , i.e. directory/project_name . The search space may contain conditional hyperparameters Below, we have a for loop creating a tunable number of layers, which themselves involve a tunable units parameter. This can be pushed to any level of parameter interdependency, including recursion. Note that all parameter names should be unique (here, in the loop over i , we name the inner parameters 'units_' + str(i) ). def build_model(hp): model = keras.Sequential() for i in range(hp.Int('num_layers', 2, 20)): model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model You can use a HyperModel subclass instead of a model-building function This makes it easy to share and reuse hypermodels. A HyperModel subclass only needs to implement a build(self, hp) method. from kerastuner import HyperModel class MyHyperModel(HyperModel): def __init__(self, num_classes): self.num_classes = num_classes def build(self, hp): model = keras.Sequential() model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(self.num_classes, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model hypermodel = MyHyperModel(num_classes=10) tuner = RandomSearch( hypermodel, objective='val_accuracy', max_trials=10, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=5, validation_data=(val_x, val_y)) Keras Tuner includes pre-made tunable applications: HyperResNet and HyperXception These are ready-to-use hypermodels for computer vision. They come pre-compiled with loss=\"categorical_crossentropy\" and metrics=[\"accuracy\"] . from kerastuner.applications import HyperResNet from kerastuner.tuners import Hyperband hypermodel = HyperResNet(input_shape=(128, 128, 3), num_classes=10) tuner = Hyperband( hypermodel, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y)) You can easily restrict the search space to just a few parameters If you have an existing hypermodel, and you want to search over only a few parameters (such as the learning rate), you can do so by passing a hyperparameters argument to the tuner constructor, as well as tune_new_entries=False to specify that parameters that you didn't list in hyperparameters should not be tuned. For these parameters, the default value gets used. from kerastuner import HyperParameters hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) hp = HyperParameters() # This will override the `learning_rate` parameter with your # own selection of choices hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]) tuner = Hyperband( hypermodel, hyperparameters=hp, # `tune_new_entries=False` prevents unlisted parameters from being tuned tune_new_entries=False, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y)) Want to know what parameter names are available? Read the code. About parameter default values Whenever you register a hyperparameter inside a model-building function or the build method of a hypermodel, you can specify a default value: hp.Int('units', min_value=32, max_value=512, step=32, default=128) If you don't, hyperparameters always have a default default (for Int , it is equal to min_value ). Fixing values in a hypermodel What if you want to do the reverse -- tune all available parameters in a hypermodel, except one (the learning rate)? Pass a hyperparameters argument with a Fixed entry (or any number of Fixed entries), and specify tune_new_entries=True . hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) hp = HyperParameters() hp.Fixed('learning_rate', value=1e-4) tuner = Hyperband( hypermodel, hyperparameters=hp, tune_new_entries=True, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y)) Overriding compilation arguments If you have a hypermodel for which you want to change the existing optimizer, loss, or metrics, you can do so by passing these arguments to the tuner constructor: hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) tuner = Hyperband( hypermodel, optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=[keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')], objective='val_precision', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y))","title":"Home"},{"location":"#keras-tuner-documentation","text":"","title":"Keras Tuner documentation"},{"location":"#installation","text":"Requirements: Python 3.6 TensorFlow 2.0 Install latest release: pip install -U keras-tuner Install from source: git clone https://github.com/keras-team/keras-tuner.git cd keras-tuner pip install .","title":"Installation"},{"location":"#usage-the-basics","text":"Here's how to perform hyperparameter tuning for a single-layer dense neural network using random search. First, we define a model-building function. It takes an argument hp from which you can sample hyperparameters, such as hp.Int('units', min_value=32, max_value=512, step=32) (an integer from a certain range). This function returns a compiled model. from tensorflow import keras from tensorflow.keras import layers from kerastuner.tuners import RandomSearch def build_model(hp): model = keras.Sequential() model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model Next, instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials ( max_trials ) to test, and the number of models that should be built and fit for each trial ( executions_per_trial ). Available tuners are RandomSearch and Hyperband . Note: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration). tuner = RandomSearch( build_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='my_dir', project_name='helloworld') You can print a summary of the search space: tuner.search_space_summary() Then, start the search for the best hyperparameter configuration. The call to search has the same signature as model.fit() . tuner.search(x, y, epochs=5, validation_data=(val_x, val_y)) Here's what happens in search : models are built iteratively by calling the model-building function, which populates the hyperparameter space (search space) tracked by the hp object. The tuner progressively explores the space, recording metrics for each configuration. When search is over, you can retrieve the best model(s): models = tuner.get_best_models(num_models=2) Or print a summary of the results: tuner.results_summary() You will also find detailed logs, checkpoints, etc, in the folder my_dir/helloworld , i.e. directory/project_name .","title":"Usage: the basics"},{"location":"#the-search-space-may-contain-conditional-hyperparameters","text":"Below, we have a for loop creating a tunable number of layers, which themselves involve a tunable units parameter. This can be pushed to any level of parameter interdependency, including recursion. Note that all parameter names should be unique (here, in the loop over i , we name the inner parameters 'units_' + str(i) ). def build_model(hp): model = keras.Sequential() for i in range(hp.Int('num_layers', 2, 20)): model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model","title":"The search space may contain conditional hyperparameters"},{"location":"#you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function","text":"This makes it easy to share and reuse hypermodels. A HyperModel subclass only needs to implement a build(self, hp) method. from kerastuner import HyperModel class MyHyperModel(HyperModel): def __init__(self, num_classes): self.num_classes = num_classes def build(self, hp): model = keras.Sequential() model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu')) model.add(layers.Dense(self.num_classes, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model hypermodel = MyHyperModel(num_classes=10) tuner = RandomSearch( hypermodel, objective='val_accuracy', max_trials=10, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=5, validation_data=(val_x, val_y))","title":"You can use a HyperModel subclass instead of a model-building function"},{"location":"#keras-tuner-includes-pre-made-tunable-applications-hyperresnet-and-hyperxception","text":"These are ready-to-use hypermodels for computer vision. They come pre-compiled with loss=\"categorical_crossentropy\" and metrics=[\"accuracy\"] . from kerastuner.applications import HyperResNet from kerastuner.tuners import Hyperband hypermodel = HyperResNet(input_shape=(128, 128, 3), num_classes=10) tuner = Hyperband( hypermodel, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y))","title":"Keras Tuner includes pre-made tunable applications: HyperResNet and HyperXception"},{"location":"#you-can-easily-restrict-the-search-space-to-just-a-few-parameters","text":"If you have an existing hypermodel, and you want to search over only a few parameters (such as the learning rate), you can do so by passing a hyperparameters argument to the tuner constructor, as well as tune_new_entries=False to specify that parameters that you didn't list in hyperparameters should not be tuned. For these parameters, the default value gets used. from kerastuner import HyperParameters hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) hp = HyperParameters() # This will override the `learning_rate` parameter with your # own selection of choices hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]) tuner = Hyperband( hypermodel, hyperparameters=hp, # `tune_new_entries=False` prevents unlisted parameters from being tuned tune_new_entries=False, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y)) Want to know what parameter names are available? Read the code.","title":"You can easily restrict the search space to just a few parameters"},{"location":"#about-parameter-default-values","text":"Whenever you register a hyperparameter inside a model-building function or the build method of a hypermodel, you can specify a default value: hp.Int('units', min_value=32, max_value=512, step=32, default=128) If you don't, hyperparameters always have a default default (for Int , it is equal to min_value ).","title":"About parameter default values"},{"location":"#fixing-values-in-a-hypermodel","text":"What if you want to do the reverse -- tune all available parameters in a hypermodel, except one (the learning rate)? Pass a hyperparameters argument with a Fixed entry (or any number of Fixed entries), and specify tune_new_entries=True . hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) hp = HyperParameters() hp.Fixed('learning_rate', value=1e-4) tuner = Hyperband( hypermodel, hyperparameters=hp, tune_new_entries=True, objective='val_accuracy', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y))","title":"Fixing values in a hypermodel"},{"location":"#overriding-compilation-arguments","text":"If you have a hypermodel for which you want to change the existing optimizer, loss, or metrics, you can do so by passing these arguments to the tuner constructor: hypermodel = HyperXception(input_shape=(128, 128, 3), num_classes=10) tuner = Hyperband( hypermodel, optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=[keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')], objective='val_precision', max_trials=40, directory='my_dir', project_name='helloworld') tuner.search(x, y, epochs=20, validation_data=(val_x, val_y))","title":"Overriding compilation arguments"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Pull Request Guide Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Make a pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request if applicable and briefly describe your contribution. For the case of bug fixes, add new test cases which would fail before your bug fix. Code Style Guide This project tries to closely follow the official Python Style Guide detailed in PEP8 . We use Flake8 to enforce it. The docstrings follow the Google Python Style Guide . Testing Guide Pytest is used to write the unit tests. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py . Pre-commit hook You can make git run flake8 before every commit automatically. It will make you go faster by avoiding pushing commit which are not passing the flake8 tests. To do this, open .git/hooks/pre-commit with a text editor and write flake8 inside. If the flake8 test doesn't pass, the commit will be aborted. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Community Guidelines This project follows Google's Open Source Community Guidelines . Rebuilding Protos If you make changes to any .proto file, you'll have to rebuild the generated *_pb2.py files. To do this, run these commands from the root directory of this project: pip install grpcio-tools python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. kerastuner/protos/kerastuner.proto python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. kerastuner/protos/service.proto","title":"Contributing Guide"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"contributing/#pull-request-guide","text":"Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Make a pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request if applicable and briefly describe your contribution. For the case of bug fixes, add new test cases which would fail before your bug fix.","title":"Pull Request Guide"},{"location":"contributing/#code-style-guide","text":"This project tries to closely follow the official Python Style Guide detailed in PEP8 . We use Flake8 to enforce it. The docstrings follow the Google Python Style Guide .","title":"Code Style Guide"},{"location":"contributing/#testing-guide","text":"Pytest is used to write the unit tests. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py .","title":"Testing Guide"},{"location":"contributing/#pre-commit-hook","text":"You can make git run flake8 before every commit automatically. It will make you go faster by avoiding pushing commit which are not passing the flake8 tests. To do this, open .git/hooks/pre-commit with a text editor and write flake8 inside. If the flake8 test doesn't pass, the commit will be aborted.","title":"Pre-commit hook"},{"location":"contributing/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"contributing/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"contributing/#rebuilding-protos","text":"If you make changes to any .proto file, you'll have to rebuild the generated *_pb2.py files. To do this, run these commands from the root directory of this project: pip install grpcio-tools python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. kerastuner/protos/kerastuner.proto python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. kerastuner/protos/service.proto","title":"Rebuilding Protos"},{"location":"documentation/hypermodels/","text":"[source] HyperModel class: kerastuner.engine.hypermodel.HyperModel(name=None, tunable=True) Defines a searchable space of Models and builds Models from this space. Attributes: name : The name of this HyperModel. tunable : Whether the hyperparameters defined in this hypermodel should be added to search space. If False , either the search space for these parameters must be defined in advance, or the default values will be used. build method: HyperModel.build(hp) Builds a model. Arguments: hp : A HyperParameters instance. Returns: A model instance. [source] HyperXception class: kerastuner.applications.xception.HyperXception(include_top=True, input_shape=None, input_tensor=None, classes=None, **kwargs) An Xception HyperModel. Arguments: include_top : whether to include the fully-connected layer at the top of the network. input_shape : Optional shape tuple, e.g. (256, 256, 3) . One of input_shape or input_tensor must be specified. input_tensor : Optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. One of input_shape or input_tensor must be specified. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. **kwargs : Additional keyword arguments that apply to all HyperModels. See kerastuner.HyperModel . [source] HyperResNet class: kerastuner.applications.resnet.HyperResNet(include_top=True, input_shape=None, input_tensor=None, classes=None, **kwargs) A ResNet HyperModel. Arguments: include_top : whether to include the fully-connected layer at the top of the network. input_shape : Optional shape tuple, e.g. (256, 256, 3) . One of input_shape or input_tensor must be specified. input_tensor : Optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. One of input_shape or input_tensor must be specified. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. **kwargs : Additional keyword arguments that apply to all HyperModels. See kerastuner.HyperModel .","title":"HyperModels"},{"location":"documentation/hypermodels/#hypermodel-class","text":"kerastuner.engine.hypermodel.HyperModel(name=None, tunable=True) Defines a searchable space of Models and builds Models from this space. Attributes: name : The name of this HyperModel. tunable : Whether the hyperparameters defined in this hypermodel should be added to search space. If False , either the search space for these parameters must be defined in advance, or the default values will be used.","title":"HyperModel class:"},{"location":"documentation/hypermodels/#build-method","text":"HyperModel.build(hp) Builds a model. Arguments: hp : A HyperParameters instance. Returns: A model instance. [source]","title":"build method:"},{"location":"documentation/hypermodels/#hyperxception-class","text":"kerastuner.applications.xception.HyperXception(include_top=True, input_shape=None, input_tensor=None, classes=None, **kwargs) An Xception HyperModel. Arguments: include_top : whether to include the fully-connected layer at the top of the network. input_shape : Optional shape tuple, e.g. (256, 256, 3) . One of input_shape or input_tensor must be specified. input_tensor : Optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. One of input_shape or input_tensor must be specified. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. **kwargs : Additional keyword arguments that apply to all HyperModels. See kerastuner.HyperModel . [source]","title":"HyperXception class:"},{"location":"documentation/hypermodels/#hyperresnet-class","text":"kerastuner.applications.resnet.HyperResNet(include_top=True, input_shape=None, input_tensor=None, classes=None, **kwargs) A ResNet HyperModel. Arguments: include_top : whether to include the fully-connected layer at the top of the network. input_shape : Optional shape tuple, e.g. (256, 256, 3) . One of input_shape or input_tensor must be specified. input_tensor : Optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. One of input_shape or input_tensor must be specified. classes : optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified. **kwargs : Additional keyword arguments that apply to all HyperModels. See kerastuner.HyperModel .","title":"HyperResNet class:"},{"location":"documentation/hyperparameters/","text":"HyperParameters The HyperParameters class serves as a hyerparameter container. A HyperParameters instance contains information about both the search space and the current values of each hyperparameter. Hyperparameters can be defined inline with the model-building code that uses them. This saves you from having to write boilerplate code and helps to make the code more maintainable. Example: Building a Model using HyperParameters import kerastuner as kt import tensorflow as tf def build_model(hp): model = tf.keras.Sequential() for i in range(hp.Int('layers', 3, 10)): model.add(tf.keras.layers.Dense( units=hp.Int('units_' + str(i), 50, 100, step=10), activation=hp.Choice('act_' + str(i), ['relu', 'tanh']))) model.add(tf.keras.layers.Dense(1, activation='sigmoid')) model.compile('adam', 'binary_crossentropy', metrics=['accuracy']) return model hp = kt.HyperParameters() model = build_model(hp) assert 'layers' in hp assert 'units_0' in hp # Restrict the search space. hp = kt.HyperParameters() hp.Fixed('layers', 5) model = build_model(hp) assert hp['layers'] == 5 # Reparametrize the search space. hp = kt.HyperParameters() hp.Int('layers', 20, 30) model = build_model(hp) assert hp['layers'] >= 20 [source] HyperParameters class: kerastuner.engine.hyperparameters.HyperParameters() Container for both a hyperparameter space, and current values. Attributes: space : A list of HyperParameter instances. values : A dict mapping hyperparameter names to current values. Boolean method: HyperParameters.Boolean(name, default=False, parent_name=None, parent_values=None) Choice between True and False. Arguments name : Str. Name of parameter. Must be unique. default : Default value to return for the parameter. If unspecified, the default value will be False. parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter. Choice method: HyperParameters.Choice(name, values, ordered=None, default=None, parent_name=None, parent_values=None) Choice of one value among a predefined set of possible values. Arguments: name : Str. Name of parameter. Must be unique. values : List of possible values. Values must be int, float, str, or bool. All values must be of the same type. ordered : Whether the values passed should be considered to have an ordering. This defaults to True for float/int values. Must be False for any other values. default : Default value to return for the parameter. If unspecified, the default value will be: None if None is one of the choices in values The first entry in values otherwise. parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter. Fixed method: HyperParameters.Fixed(name, value, parent_name=None, parent_values=None) Fixed, untunable value. Arguments name : Str. Name of parameter. Must be unique. value : Value to use (can be any JSON-serializable Python type). parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter. Float method: HyperParameters.Float(name, min_value, max_value, step=None, sampling=None, default=None, parent_name=None, parent_values=None) Floating point range, can be evenly divided. Arguments: name : Str. Name of parameter. Must be unique. min_value : Float. Lower bound of the range. max_value : Float. Upper bound of the range. step : Optional. Float, e.g. 0.1. smallest meaningful distance between two values. Whether step should be specified is Oracle dependent, since some Oracles can infer an optimal step automatically. sampling : Optional. One of \"linear\", \"log\", \"reverse_log\". Acts as a hint for an initial prior probability distribution for how this value should be sampled, e.g. \"log\" will assign equal probabilities to each order of magnitude range. default : Default value to return for the parameter. If unspecified, the default value will be min_value . parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter. Int method: HyperParameters.Int(name, min_value, max_value, step=1, sampling=None, default=None, parent_name=None, parent_values=None) Integer range. Note that unlinke Python's range function, max_value is included in the possible values this parameter can take on. Arguments: name : Str. Name of parameter. Must be unique. min_value : Int. Lower limit of range (included). max_value : Int. Upper limit of range (included). step : Int. Step of range. sampling : Optional. One of \"linear\", \"log\", \"reverse_log\". Acts as a hint for an initial prior probability distribution for how this value should be sampled, e.g. \"log\" will assign equal probabilities to each order of magnitude range. default : Default value to return for the parameter. If unspecified, the default value will be min_value . parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter. conditional_scope method: HyperParameters.conditional_scope(parent_name, parent_values) Opens a scope to create conditional HyperParameters. All HyperParameters created under this scope will only be active when the parent HyperParameter specified by parent_name is equal to one of the values passed in parent_values . When the condition is not met, creating a HyperParameter under this scope will register the HyperParameter, but will return None rather than a concrete value. Note that any Python code under this scope will execute regardless of whether the condition is met. Arguments: parent_name : The name of the HyperParameter to condition on. parent_values : Values of the parent HyperParameter for which HyperParameters under this scope should be considered valid. get method: HyperParameters.get(name) Return the current value of this HyperParameter.","title":"HyperParameters"},{"location":"documentation/hyperparameters/#hyperparameters","text":"The HyperParameters class serves as a hyerparameter container. A HyperParameters instance contains information about both the search space and the current values of each hyperparameter. Hyperparameters can be defined inline with the model-building code that uses them. This saves you from having to write boilerplate code and helps to make the code more maintainable.","title":"HyperParameters"},{"location":"documentation/hyperparameters/#example-building-a-model-using-hyperparameters","text":"import kerastuner as kt import tensorflow as tf def build_model(hp): model = tf.keras.Sequential() for i in range(hp.Int('layers', 3, 10)): model.add(tf.keras.layers.Dense( units=hp.Int('units_' + str(i), 50, 100, step=10), activation=hp.Choice('act_' + str(i), ['relu', 'tanh']))) model.add(tf.keras.layers.Dense(1, activation='sigmoid')) model.compile('adam', 'binary_crossentropy', metrics=['accuracy']) return model hp = kt.HyperParameters() model = build_model(hp) assert 'layers' in hp assert 'units_0' in hp # Restrict the search space. hp = kt.HyperParameters() hp.Fixed('layers', 5) model = build_model(hp) assert hp['layers'] == 5 # Reparametrize the search space. hp = kt.HyperParameters() hp.Int('layers', 20, 30) model = build_model(hp) assert hp['layers'] >= 20 [source]","title":"Example: Building a Model using HyperParameters"},{"location":"documentation/hyperparameters/#hyperparameters-class","text":"kerastuner.engine.hyperparameters.HyperParameters() Container for both a hyperparameter space, and current values. Attributes: space : A list of HyperParameter instances. values : A dict mapping hyperparameter names to current values.","title":"HyperParameters class:"},{"location":"documentation/hyperparameters/#boolean-method","text":"HyperParameters.Boolean(name, default=False, parent_name=None, parent_values=None) Choice between True and False. Arguments name : Str. Name of parameter. Must be unique. default : Default value to return for the parameter. If unspecified, the default value will be False. parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter.","title":"Boolean method:"},{"location":"documentation/hyperparameters/#choice-method","text":"HyperParameters.Choice(name, values, ordered=None, default=None, parent_name=None, parent_values=None) Choice of one value among a predefined set of possible values. Arguments: name : Str. Name of parameter. Must be unique. values : List of possible values. Values must be int, float, str, or bool. All values must be of the same type. ordered : Whether the values passed should be considered to have an ordering. This defaults to True for float/int values. Must be False for any other values. default : Default value to return for the parameter. If unspecified, the default value will be: None if None is one of the choices in values The first entry in values otherwise. parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter.","title":"Choice method:"},{"location":"documentation/hyperparameters/#fixed-method","text":"HyperParameters.Fixed(name, value, parent_name=None, parent_values=None) Fixed, untunable value. Arguments name : Str. Name of parameter. Must be unique. value : Value to use (can be any JSON-serializable Python type). parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter.","title":"Fixed method:"},{"location":"documentation/hyperparameters/#float-method","text":"HyperParameters.Float(name, min_value, max_value, step=None, sampling=None, default=None, parent_name=None, parent_values=None) Floating point range, can be evenly divided. Arguments: name : Str. Name of parameter. Must be unique. min_value : Float. Lower bound of the range. max_value : Float. Upper bound of the range. step : Optional. Float, e.g. 0.1. smallest meaningful distance between two values. Whether step should be specified is Oracle dependent, since some Oracles can infer an optimal step automatically. sampling : Optional. One of \"linear\", \"log\", \"reverse_log\". Acts as a hint for an initial prior probability distribution for how this value should be sampled, e.g. \"log\" will assign equal probabilities to each order of magnitude range. default : Default value to return for the parameter. If unspecified, the default value will be min_value . parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter.","title":"Float method:"},{"location":"documentation/hyperparameters/#int-method","text":"HyperParameters.Int(name, min_value, max_value, step=1, sampling=None, default=None, parent_name=None, parent_values=None) Integer range. Note that unlinke Python's range function, max_value is included in the possible values this parameter can take on. Arguments: name : Str. Name of parameter. Must be unique. min_value : Int. Lower limit of range (included). max_value : Int. Upper limit of range (included). step : Int. Step of range. sampling : Optional. One of \"linear\", \"log\", \"reverse_log\". Acts as a hint for an initial prior probability distribution for how this value should be sampled, e.g. \"log\" will assign equal probabilities to each order of magnitude range. default : Default value to return for the parameter. If unspecified, the default value will be min_value . parent_name: (Optional) String. Specifies that this hyperparameter is conditional. The name of the this hyperparameter's parent. parent_values: (Optional) List. The values of the parent hyperparameter for which this hyperparameter should be considered active. Returns: The current value of this hyperparameter.","title":"Int method:"},{"location":"documentation/hyperparameters/#conditional_scope-method","text":"HyperParameters.conditional_scope(parent_name, parent_values) Opens a scope to create conditional HyperParameters. All HyperParameters created under this scope will only be active when the parent HyperParameter specified by parent_name is equal to one of the values passed in parent_values . When the condition is not met, creating a HyperParameter under this scope will register the HyperParameter, but will return None rather than a concrete value. Note that any Python code under this scope will execute regardless of whether the condition is met. Arguments: parent_name : The name of the HyperParameter to condition on. parent_values : Values of the parent HyperParameter for which HyperParameters under this scope should be considered valid.","title":"conditional_scope method:"},{"location":"documentation/hyperparameters/#get-method","text":"HyperParameters.get(name) Return the current value of this HyperParameter.","title":"get method:"},{"location":"documentation/oracles/","text":"Oracles Each Oracle class implements a particular hyperparameter tuning algorithm. An Oracle is passed as an argument to a Tuner . The Oracle tells the Tuner which hyperparameters should be tried next. Most Oracle classes can be combined with any user-defined Tuner subclass. Hyperband requires the Tuner class to implement additional Oracle -specific functionality (see Hyperband documentation). If you do not need to subclass Tuner (the most common case), we also provide a number of convenience classes that package a Tuner and an Oracle together (e.g kerastuner.RandomSearch , kerastuner.BayesianOptimization , and kerastuner.Hyperband ). [source] BayesianOptimizationOracle class: kerastuner.oracles.BayesianOptimization(objective, max_trials, num_initial_points=None, alpha=0.0001, beta=2.6, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Bayesian optimization oracle. It uses Bayesian optimization with a underlying Gaussian process model. The acquisition function used is upper confidence bound (UCB), which can be found in the following link: https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf Arguments objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted. num_initial_points : (Optional) Int. The number of randomly generated samples as initial training data for Bayesian optimization. If not specified, a value of 3 times the dimensionality of the hyperparameter space is used. alpha : Float. Value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. beta : Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source] HyperbandOracle class: kerastuner.oracles.Hyperband(objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Oracle class for Hyperband. Note that to use this Oracle with your own subclassed Tuner, your Tuner class must be able to handle in Tuner.run_trial three special hyperparameters that will be set by this Tuner: \"tuner/trial_id\": String, optionally set. The trial_id of the Trial to load from when starting this trial. \"tuner/initial_epoch\": Int, always set. The initial epoch the Trial should be started from. \"tuner/epochs\": Int, always set. The cumulative number of epochs this Trial should be trained. These hyperparameters will be set during the \"successive halving\" portion of the Hyperband algorithm. Example run_trial : def run_trial(self, trial, *args, **kwargs): hp = trial.hyperparameters if \"tuner/trial_id\" in hp: past_trial = self.oracle.get_trial(hp['tuner/trial_id']) model = self.load_model(past_trial) else: model = self.hypermodel.build(hp) initial_epoch = hp['tuner/initial_epoch'] last_epoch = hp['tuner/epochs'] for epoch in range(initial_epoch, last_epoch): self.on_epoch_begin(...) for step in range(...): # Run model training step here. self.on_epoch_end(...) Arguments: objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_epochs : Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during training (for example, via tf.keras.callbacks.EarlyStopping ). factor : Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations : Int >= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source] RandomSearchOracle class: kerastuner.oracles.RandomSearch(objective, max_trials, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Random search oracle. Arguments: objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source] Oracle class: kerastuner.engine.oracle.Oracle(objective, max_trials=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Implements a hyperparameter optimization algorithm. Attributes: objective: String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials: The maximum number of hyperparameter combinations to try. hyperparameters: HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries: Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries: Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . _populate_space method: Oracle._populate_space(trial_id) Fill the hyperparameter space with values for a trial. This method should be overrridden in subclasses and called in create_trial in order to populate the hyperparameter space with values. Args: trial_id : The id for this Trial. Returns: A dictionary with keys \"values\" and \"status\", where \"values\" is a mapping of parameter names to suggested values, and \"status\" is the TrialStatus that should be returned for this trial (one of \"RUNNING\", \"IDLE\", or \"STOPPED\"). _score_trial method: Oracle._score_trial(trial) Score a completed Trial . This method can be overridden in subclasses to provide a score for a set of hyperparameter values. This method is called from end_trial on completed Trial s. Args: trial: A completed Trial object. create_trial method: Oracle.create_trial(tuner_id) Create a new Trial to be run by the Tuner . A Trial corresponds to a unique set of hyperparameters to be run by Tuner.run_trial . Args: tuner_id: A ID that identifies the Tuner requesting a Trial . Tuners that should run the same trial (for instance, when running a multi-worker model) should have the same ID. Returns: A Trial object containing a set of hyperparameter values to run in a Tuner . end_trial method: Oracle.end_trial(trial_id, status='COMPLETED') Record the measured objective for a set of parameter values. Args: trial_id: String. Unique id for this trial. status: String, one of \"COMPLETED\", \"INVALID\". A status of \"INVALID\" means a trial has crashed or been deemed infeasible. get_best_trials method: Oracle.get_best_trials(num_trials=1) Returns the best Trial s. get_state method: Oracle.get_state() Returns the current state of this object. This method is called during save . set_state method: Oracle.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object. update_trial method: Oracle.update_trial(trial_id, metrics, step=0) Used by a worker to report the status of a trial. Args: trial_id: A previously seen trial id. metrics: Dict of float. The current value of this trial's metrics. step: (Optional) Float. Used to report intermediate results. The current value in a timeseries representing the state of the trial. This is the value that metrics will be associated with. Returns: Trial object. Trial.status will be set to \"STOPPED\" if the Trial should be stopped early.","title":"Oracles"},{"location":"documentation/oracles/#oracles","text":"Each Oracle class implements a particular hyperparameter tuning algorithm. An Oracle is passed as an argument to a Tuner . The Oracle tells the Tuner which hyperparameters should be tried next. Most Oracle classes can be combined with any user-defined Tuner subclass. Hyperband requires the Tuner class to implement additional Oracle -specific functionality (see Hyperband documentation). If you do not need to subclass Tuner (the most common case), we also provide a number of convenience classes that package a Tuner and an Oracle together (e.g kerastuner.RandomSearch , kerastuner.BayesianOptimization , and kerastuner.Hyperband ). [source]","title":"Oracles"},{"location":"documentation/oracles/#bayesianoptimizationoracle-class","text":"kerastuner.oracles.BayesianOptimization(objective, max_trials, num_initial_points=None, alpha=0.0001, beta=2.6, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Bayesian optimization oracle. It uses Bayesian optimization with a underlying Gaussian process model. The acquisition function used is upper confidence bound (UCB), which can be found in the following link: https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf Arguments objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted. num_initial_points : (Optional) Int. The number of randomly generated samples as initial training data for Bayesian optimization. If not specified, a value of 3 times the dimensionality of the hyperparameter space is used. alpha : Float. Value added to the diagonal of the kernel matrix during fitting. It represents the expected amount of noise in the observed performances in Bayesian optimization. beta : Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source]","title":"BayesianOptimizationOracle class:"},{"location":"documentation/oracles/#hyperbandoracle-class","text":"kerastuner.oracles.Hyperband(objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Oracle class for Hyperband. Note that to use this Oracle with your own subclassed Tuner, your Tuner class must be able to handle in Tuner.run_trial three special hyperparameters that will be set by this Tuner: \"tuner/trial_id\": String, optionally set. The trial_id of the Trial to load from when starting this trial. \"tuner/initial_epoch\": Int, always set. The initial epoch the Trial should be started from. \"tuner/epochs\": Int, always set. The cumulative number of epochs this Trial should be trained. These hyperparameters will be set during the \"successive halving\" portion of the Hyperband algorithm. Example run_trial : def run_trial(self, trial, *args, **kwargs): hp = trial.hyperparameters if \"tuner/trial_id\" in hp: past_trial = self.oracle.get_trial(hp['tuner/trial_id']) model = self.load_model(past_trial) else: model = self.hypermodel.build(hp) initial_epoch = hp['tuner/initial_epoch'] last_epoch = hp['tuner/epochs'] for epoch in range(initial_epoch, last_epoch): self.on_epoch_begin(...) for step in range(...): # Run model training step here. self.on_epoch_end(...) Arguments: objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_epochs : Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected epochs to convergence for your largest Model, and to use early stopping during training (for example, via tf.keras.callbacks.EarlyStopping ). factor : Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations : Int >= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source]","title":"HyperbandOracle class:"},{"location":"documentation/oracles/#randomsearchoracle-class","text":"kerastuner.oracles.RandomSearch(objective, max_trials, seed=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Random search oracle. Arguments: objective : String or kerastuner.Objective . If a string, the direction of the optimization (min or max) will be inferred. max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . [source]","title":"RandomSearchOracle class:"},{"location":"documentation/oracles/#oracle-class","text":"kerastuner.engine.oracle.Oracle(objective, max_trials=None, hyperparameters=None, allow_new_entries=True, tune_new_entries=True) Implements a hyperparameter optimization algorithm. Attributes: objective: String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials: The maximum number of hyperparameter combinations to try. hyperparameters: HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries: Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries: Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters .","title":"Oracle class:"},{"location":"documentation/oracles/#_populate_space-method","text":"Oracle._populate_space(trial_id) Fill the hyperparameter space with values for a trial. This method should be overrridden in subclasses and called in create_trial in order to populate the hyperparameter space with values. Args: trial_id : The id for this Trial. Returns: A dictionary with keys \"values\" and \"status\", where \"values\" is a mapping of parameter names to suggested values, and \"status\" is the TrialStatus that should be returned for this trial (one of \"RUNNING\", \"IDLE\", or \"STOPPED\").","title":"_populate_space method:"},{"location":"documentation/oracles/#_score_trial-method","text":"Oracle._score_trial(trial) Score a completed Trial . This method can be overridden in subclasses to provide a score for a set of hyperparameter values. This method is called from end_trial on completed Trial s. Args: trial: A completed Trial object.","title":"_score_trial method:"},{"location":"documentation/oracles/#create_trial-method","text":"Oracle.create_trial(tuner_id) Create a new Trial to be run by the Tuner . A Trial corresponds to a unique set of hyperparameters to be run by Tuner.run_trial . Args: tuner_id: A ID that identifies the Tuner requesting a Trial . Tuners that should run the same trial (for instance, when running a multi-worker model) should have the same ID. Returns: A Trial object containing a set of hyperparameter values to run in a Tuner .","title":"create_trial method:"},{"location":"documentation/oracles/#end_trial-method","text":"Oracle.end_trial(trial_id, status='COMPLETED') Record the measured objective for a set of parameter values. Args: trial_id: String. Unique id for this trial. status: String, one of \"COMPLETED\", \"INVALID\". A status of \"INVALID\" means a trial has crashed or been deemed infeasible.","title":"end_trial method:"},{"location":"documentation/oracles/#get_best_trials-method","text":"Oracle.get_best_trials(num_trials=1) Returns the best Trial s.","title":"get_best_trials method:"},{"location":"documentation/oracles/#get_state-method","text":"Oracle.get_state() Returns the current state of this object. This method is called during save .","title":"get_state method:"},{"location":"documentation/oracles/#set_state-method","text":"Oracle.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object.","title":"set_state method:"},{"location":"documentation/oracles/#update_trial-method","text":"Oracle.update_trial(trial_id, metrics, step=0) Used by a worker to report the status of a trial. Args: trial_id: A previously seen trial id. metrics: Dict of float. The current value of this trial's metrics. step: (Optional) Float. Used to report intermediate results. The current value in a timeseries representing the state of the trial. This is the value that metrics will be associated with. Returns: Trial object. Trial.status will be set to \"STOPPED\" if the Trial should be stopped early.","title":"update_trial method:"},{"location":"documentation/tuners/","text":"Tuners Tuners are here to do the hyperparameter search. You can create custom Tuners by subclassing kerastuner.engine.tuner.Tuner . [source] BayesianOptimization class: kerastuner.tuners.bayesian.BayesianOptimization(hypermodel, objective, max_trials, num_initial_points=2, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) BayesianOptimization tuning with Gaussian process. Arguments: hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted. num_initial_points : Int. The number of randomly generated samples as initial training data for Bayesian optimization. alpha : Float or array-like. Value added to the diagonal of the kernel matrix during fitting. beta : Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source] Hyperband class: kerastuner.tuners.hyperband.Hyperband(hypermodel, objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) Variation of HyperBand algorithm. Reference: Li, Lisha, and Kevin Jamieson. \"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.\" Journal of Machine Learning Research 18 (2018): 1-52 . Arguments hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_epochs : Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected time to convergence for your largest Model, and to use early stopping during training (for example, via tf.keras.callbacks.EarlyStopping ). factor : Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations : Int >= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source] RandomSearch class: kerastuner.tuners.randomsearch.RandomSearch(hypermodel, objective, max_trials, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) Random search tuner. Arguments: hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source] Sklearn class: kerastuner.tuners.sklearn.Sklearn(oracle, hypermodel, scoring=None, metrics=None, cv=KFold(n_splits=5, random_state=1, shuffle=True), **kwargs) Tuner for Scikit-learn Models. Performs cross-validated hyperparameter search for Scikit-learn models. Arguments: oracle : An instance of the kerastuner.Oracle class. Note that for this Tuner , the objective for the Oracle should always be set to Objective('score', direction='max') . Also, Oracle s that exploit Neural-Network-specific training (e.g. Hyperband ) should not be used with this Tuner . hypermodel : Instance of HyperModel class (or callable that takes a Hyperparameters object and returns a Model instance). scoring : An sklearn scoring function. For more information, see sklearn.metrics.make_scorer . If not provided, the Model's default scoring will be used via model.score . Note that if you are searching across different Model families, the default scoring for these Models will often be different. In this case you should supply scoring here in order to make sure your Models are being scored on the same metric. metrics : Additional sklearn.metrics functions to monitor during search. Note that these metrics do not affect the search process. cv : An sklearn.model_selection Splitter class. Used to determine how samples are split up into groups for cross-validation. **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . Example: import kerastuner as kt from sklearn import ensemble from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection def build_model(hp): model_type = hp.Choice('model_type', ['random_forest', 'ridge']) if model_type == 'random_forest': model = ensemble.RandomForestClassifier( n_estimators=hp.Int('n_estimators', 10, 50, step=10), max_depth=hp.Int('max_depth', 3, 10)) else: model = linear_model.RidgeClassifier( alpha=hp.Float('alpha', 1e-3, 1, sampling='log')) return model tuner = kt.tuners.Sklearn( oracle=kt.oracles.BayesianOptimization( objective=kt.Objective('score', 'max'), max_trials=10), hypermodel=build_model, scoring=metrics.make_scorer(metrics.accuracy_score), cv=model_selection.StratifiedKFold(5), directory='.', project_name='my_project') X, y = datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = model_selection.train_test_split( X, y, test_size=0.2) tuner.search(X_train, y_train) best_model = tuner.get_best_models(num_models=1)[0] [source] Tuner class: kerastuner.engine.tuner.Tuner(oracle, hypermodel, max_model_size=None, optimizer=None, loss=None, metrics=None, distribution_strategy=None, directory=None, project_name=None, logger=None, tuner_id=None, overwrite=False) Tuner class for Keras models. May be subclassed to create new tuners. Arguments: oracle : Instance of Oracle class. hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). max_model_size : Int. Maximum size of weights (in floating point coefficients) for a valid models. Models larger than this are rejected. optimizer : Optional. Optimizer instance. May be used to override the optimizer argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. loss : Optional. May be used to override the loss argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. metrics : Optional. May be used to override the metrics argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. distribution_strategy : Optional. A TensorFlow tf.distribute DistributionStrategy instance. If specified, each trial will run under this scope. For example, tf.distribute.MirroredStrategy(['/gpu:0, /'gpu:1]) will run each trial on two GPUs. Currently only single-worker strategies are supported. directory : String. Path to the working directory (relative). project_name : Name to use as prefix for files saved by this Tuner. logger : Optional. Instance of Logger class, used for streaming data to Cloud Service for monitoring. overwrite : Bool, default False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. get_best_models method: Tuner.get_best_models(num_models=1) Returns the best model(s), as determined by the tuner's objective. The models are loaded with the weights corresponding to their best checkpoint (at the end of the best epoch of best trial). This method is only a convenience shortcut. For best performance, It is recommended to retrain your Model on the full dataset using the best hyperparameters found during search . Args: num_models (int, optional): Number of best models to return. Models will be returned in sorted order. Defaults to 1. Returns: List of trained model instances. get_state method: BaseTuner.get_state() Returns the current state of this object. This method is called during save . load_model method: Tuner.load_model(trial) Loads a Model from a given trial. Arguments: trial : A Trial instance. For models that report intermediate results to the Oracle , generally load_model should load the best reported step by relying of trial.best_step on_epoch_begin method: Tuner.on_epoch_begin(trial, model, epoch, logs=None) A hook called at the start of every epoch. Arguments: trial : A Trial instance. model : A Keras Model . epoch : The current epoch number. logs : Additional metrics. on_batch_begin method: Tuner.on_batch_begin(trial, model, batch, logs) A hook called at the start of every batch. Arguments: trial : A Trial instance. model : A Keras Model . batch : The current batch number within the curent epoch. logs : Additional metrics. on_batch_end method: Tuner.on_batch_end(trial, model, batch, logs=None) A hook called at the end of every batch. Arguments: trial : A Trial instance. model : A Keras Model . batch : The current batch number within the curent epoch. logs : Additional metrics. on_epoch_end method: Tuner.on_epoch_end(trial, model, epoch, logs=None) A hook called at the end of every epoch. Arguments: trial : A Trial instance. model : A Keras Model . epoch : The current epoch number. logs : Dict. Metrics for this epoch. This should include the value of the objective for this epoch. run_trial method: Tuner.run_trial(trial, *fit_args, **fit_kwargs) Evaluates a set of hyperparameter values. This method is called during search to evaluate a set of hyperparameters. Arguments: trial : A Trial instance that contains the information needed to run this trial. Hyperparameters can be accessed via trial.hyperparameters . *fit_args : Positional arguments passed by search . *fit_kwargs : Keyword arguments passed by search . save_model method: Tuner.save_model(trial_id, model, step=0) Saves a Model for a given trial. Arguments: trial_id : The ID of the Trial that corresponds to this Model. model : The trained model. step : For models that report intermediate results to the Oracle , the step that this saved file should correspond to. For example, for Keras models this is the number of epochs trained. search method: BaseTuner.search(*fit_args, **fit_kwargs) Performs a search for best hyperparameter configuations. Arguments: *fit_args : Positional arguments that should be passed to run_trial , for example the training and validation data. *fit_kwargs : Keyword arguments that should be passed to run_trial , for example the training and validation data. set_state method: BaseTuner.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object. [source] BaseTuner class: kerastuner.engine.base_tuner.BaseTuner(oracle, hypermodel, directory=None, project_name=None, logger=None, overwrite=False) Tuner base class. May be subclassed to create new tuners, including for non-Keras models. Arguments: oracle : Instance of Oracle class. hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). directory : String. Path to the working directory (relative). project_name : Name to use as prefix for files saved by this Tuner. logger : Optional. Instance of Logger class, used for streaming data to Cloud Service for monitoring. overwrite : Bool, default False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. get_best_hyperparameters method: BaseTuner.get_best_hyperparameters(num_trials=1) Returns the best hyperparameters, as determined by the objective. This method can be used to reinstantiate the (untrained) best model found during the search process. Example: best_hp = tuner.get_best_hyperparameters()[0] model = tuner.hypermodel.build(best_hp) Arguments: num_trials : (int, optional). Number of HyperParameters objects to return. HyperParameters will be returned in sorted order based on trial performance. Returns: List of HyperParameter objects. get_best_models method: BaseTuner.get_best_models(num_models=1) Returns the best model(s), as determined by the objective. This method is only a convenience shortcut. For best performance, It is recommended to retrain your Model on the full dataset using the best hyperparameters found during search . Arguments: num_models (int, optional). Number of best models to return. Models will be returned in sorted order. Defaults to 1. Returns: List of trained model instances. get_state method: BaseTuner.get_state() Returns the current state of this object. This method is called during save . load_model method: BaseTuner.load_model(trial) Loads a Model from a given trial. Arguments: trial : A Trial instance. For models that report intermediate results to the Oracle , generally load_model should load the best reported step by relying of trial.best_step run_trial method: BaseTuner.run_trial(trial, *fit_args, **fit_kwargs) Evaluates a set of hyperparameter values. This method is called during search to evaluate a set of hyperparameters. For subclass implementers: This method is responsible for reporting metrics related to the Trial to the Oracle via self.oracle.update_trial . Simplest example: def run_trial(self, trial, x, y, val_x, val_y): model = self.hypermodel.build(trial.hyperparameters) model.fit(x, y) loss = model.evaluate(val_x, val_y) self.oracle.update_trial( trial.trial_id, {'loss': loss}) self.save_model(trial.trial_id, model) Arguments: trial : A Trial instance that contains the information needed to run this trial. Hyperparameters can be accessed via trial.hyperparameters . *fit_args : Positional arguments passed by search . *fit_kwargs : Keyword arguments passed by search . save_model method: BaseTuner.save_model(trial_id, model, step=0) Saves a Model for a given trial. Arguments: trial_id : The ID of the Trial that corresponds to this Model. model : The trained model. step : For models that report intermediate results to the Oracle , the step that this saved file should correspond to. For example, for Keras models this is the number of epochs trained. search method: BaseTuner.search(*fit_args, **fit_kwargs) Performs a search for best hyperparameter configuations. Arguments: *fit_args : Positional arguments that should be passed to run_trial , for example the training and validation data. *fit_kwargs : Keyword arguments that should be passed to run_trial , for example the training and validation data. set_state method: BaseTuner.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object.","title":"Tuners"},{"location":"documentation/tuners/#tuners","text":"Tuners are here to do the hyperparameter search. You can create custom Tuners by subclassing kerastuner.engine.tuner.Tuner . [source]","title":"Tuners"},{"location":"documentation/tuners/#bayesianoptimization-class","text":"kerastuner.tuners.bayesian.BayesianOptimization(hypermodel, objective, max_trials, num_initial_points=2, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) BayesianOptimization tuning with Gaussian process. Arguments: hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested if the search space has been exhausted. num_initial_points : Int. The number of randomly generated samples as initial training data for Bayesian optimization. alpha : Float or array-like. Value added to the diagonal of the kernel matrix during fitting. beta : Float. The balancing factor of exploration and exploitation. The larger it is, the more explorative it is. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source]","title":"BayesianOptimization class:"},{"location":"documentation/tuners/#hyperband-class","text":"kerastuner.tuners.hyperband.Hyperband(hypermodel, objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) Variation of HyperBand algorithm. Reference: Li, Lisha, and Kevin Jamieson. \"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.\" Journal of Machine Learning Research 18 (2018): 1-52 . Arguments hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_epochs : Int. The maximum number of epochs to train one model. It is recommended to set this to a value slightly higher than the expected time to convergence for your largest Model, and to use early stopping during training (for example, via tf.keras.callbacks.EarlyStopping ). factor : Int. Reduction factor for the number of epochs and number of models for each bracket. hyperband_iterations : Int >= 1. The number of times to iterate over the full Hyperband algorithm. One iteration will run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is recommended to set this to as high a value as is within your resource budget. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source]","title":"Hyperband class:"},{"location":"documentation/tuners/#randomsearch-class","text":"kerastuner.tuners.randomsearch.RandomSearch(hypermodel, objective, max_trials, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs) Random search tuner. Arguments: hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). objective : String. Name of model metric to minimize or maximize, e.g. \"val_accuracy\". max_trials : Int. Total number of trials (model configurations) to test at most. Note that the oracle may interrupt the search before max_trial models have been tested. seed : Int. Random seed. hyperparameters : HyperParameters class instance. Can be used to override (or register in advance) hyperparamters in the search space. tune_new_entries : Whether hyperparameter entries that are requested by the hypermodel but that were not specified in hyperparameters should be added to the search space, or not. If not, then the default value for these parameters will be used. allow_new_entries : Whether the hypermodel is allowed to request hyperparameter entries not listed in hyperparameters . **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . [source]","title":"RandomSearch class:"},{"location":"documentation/tuners/#sklearn-class","text":"kerastuner.tuners.sklearn.Sklearn(oracle, hypermodel, scoring=None, metrics=None, cv=KFold(n_splits=5, random_state=1, shuffle=True), **kwargs) Tuner for Scikit-learn Models. Performs cross-validated hyperparameter search for Scikit-learn models. Arguments: oracle : An instance of the kerastuner.Oracle class. Note that for this Tuner , the objective for the Oracle should always be set to Objective('score', direction='max') . Also, Oracle s that exploit Neural-Network-specific training (e.g. Hyperband ) should not be used with this Tuner . hypermodel : Instance of HyperModel class (or callable that takes a Hyperparameters object and returns a Model instance). scoring : An sklearn scoring function. For more information, see sklearn.metrics.make_scorer . If not provided, the Model's default scoring will be used via model.score . Note that if you are searching across different Model families, the default scoring for these Models will often be different. In this case you should supply scoring here in order to make sure your Models are being scored on the same metric. metrics : Additional sklearn.metrics functions to monitor during search. Note that these metrics do not affect the search process. cv : An sklearn.model_selection Splitter class. Used to determine how samples are split up into groups for cross-validation. **kwargs : Keyword arguments relevant to all Tuner subclasses. Please see the docstring for Tuner . Example: import kerastuner as kt from sklearn import ensemble from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection def build_model(hp): model_type = hp.Choice('model_type', ['random_forest', 'ridge']) if model_type == 'random_forest': model = ensemble.RandomForestClassifier( n_estimators=hp.Int('n_estimators', 10, 50, step=10), max_depth=hp.Int('max_depth', 3, 10)) else: model = linear_model.RidgeClassifier( alpha=hp.Float('alpha', 1e-3, 1, sampling='log')) return model tuner = kt.tuners.Sklearn( oracle=kt.oracles.BayesianOptimization( objective=kt.Objective('score', 'max'), max_trials=10), hypermodel=build_model, scoring=metrics.make_scorer(metrics.accuracy_score), cv=model_selection.StratifiedKFold(5), directory='.', project_name='my_project') X, y = datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = model_selection.train_test_split( X, y, test_size=0.2) tuner.search(X_train, y_train) best_model = tuner.get_best_models(num_models=1)[0] [source]","title":"Sklearn class:"},{"location":"documentation/tuners/#tuner-class","text":"kerastuner.engine.tuner.Tuner(oracle, hypermodel, max_model_size=None, optimizer=None, loss=None, metrics=None, distribution_strategy=None, directory=None, project_name=None, logger=None, tuner_id=None, overwrite=False) Tuner class for Keras models. May be subclassed to create new tuners. Arguments: oracle : Instance of Oracle class. hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). max_model_size : Int. Maximum size of weights (in floating point coefficients) for a valid models. Models larger than this are rejected. optimizer : Optional. Optimizer instance. May be used to override the optimizer argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. loss : Optional. May be used to override the loss argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. metrics : Optional. May be used to override the metrics argument in the compile step for the models. If the hypermodel does not compile the models it generates, then this argument must be specified. distribution_strategy : Optional. A TensorFlow tf.distribute DistributionStrategy instance. If specified, each trial will run under this scope. For example, tf.distribute.MirroredStrategy(['/gpu:0, /'gpu:1]) will run each trial on two GPUs. Currently only single-worker strategies are supported. directory : String. Path to the working directory (relative). project_name : Name to use as prefix for files saved by this Tuner. logger : Optional. Instance of Logger class, used for streaming data to Cloud Service for monitoring. overwrite : Bool, default False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project.","title":"Tuner class:"},{"location":"documentation/tuners/#get_best_models-method","text":"Tuner.get_best_models(num_models=1) Returns the best model(s), as determined by the tuner's objective. The models are loaded with the weights corresponding to their best checkpoint (at the end of the best epoch of best trial). This method is only a convenience shortcut. For best performance, It is recommended to retrain your Model on the full dataset using the best hyperparameters found during search . Args: num_models (int, optional): Number of best models to return. Models will be returned in sorted order. Defaults to 1. Returns: List of trained model instances.","title":"get_best_models method:"},{"location":"documentation/tuners/#get_state-method","text":"BaseTuner.get_state() Returns the current state of this object. This method is called during save .","title":"get_state method:"},{"location":"documentation/tuners/#load_model-method","text":"Tuner.load_model(trial) Loads a Model from a given trial. Arguments: trial : A Trial instance. For models that report intermediate results to the Oracle , generally load_model should load the best reported step by relying of trial.best_step","title":"load_model method:"},{"location":"documentation/tuners/#on_epoch_begin-method","text":"Tuner.on_epoch_begin(trial, model, epoch, logs=None) A hook called at the start of every epoch. Arguments: trial : A Trial instance. model : A Keras Model . epoch : The current epoch number. logs : Additional metrics.","title":"on_epoch_begin method:"},{"location":"documentation/tuners/#on_batch_begin-method","text":"Tuner.on_batch_begin(trial, model, batch, logs) A hook called at the start of every batch. Arguments: trial : A Trial instance. model : A Keras Model . batch : The current batch number within the curent epoch. logs : Additional metrics.","title":"on_batch_begin method:"},{"location":"documentation/tuners/#on_batch_end-method","text":"Tuner.on_batch_end(trial, model, batch, logs=None) A hook called at the end of every batch. Arguments: trial : A Trial instance. model : A Keras Model . batch : The current batch number within the curent epoch. logs : Additional metrics.","title":"on_batch_end method:"},{"location":"documentation/tuners/#on_epoch_end-method","text":"Tuner.on_epoch_end(trial, model, epoch, logs=None) A hook called at the end of every epoch. Arguments: trial : A Trial instance. model : A Keras Model . epoch : The current epoch number. logs : Dict. Metrics for this epoch. This should include the value of the objective for this epoch.","title":"on_epoch_end method:"},{"location":"documentation/tuners/#run_trial-method","text":"Tuner.run_trial(trial, *fit_args, **fit_kwargs) Evaluates a set of hyperparameter values. This method is called during search to evaluate a set of hyperparameters. Arguments: trial : A Trial instance that contains the information needed to run this trial. Hyperparameters can be accessed via trial.hyperparameters . *fit_args : Positional arguments passed by search . *fit_kwargs : Keyword arguments passed by search .","title":"run_trial method:"},{"location":"documentation/tuners/#save_model-method","text":"Tuner.save_model(trial_id, model, step=0) Saves a Model for a given trial. Arguments: trial_id : The ID of the Trial that corresponds to this Model. model : The trained model. step : For models that report intermediate results to the Oracle , the step that this saved file should correspond to. For example, for Keras models this is the number of epochs trained.","title":"save_model method:"},{"location":"documentation/tuners/#search-method","text":"BaseTuner.search(*fit_args, **fit_kwargs) Performs a search for best hyperparameter configuations. Arguments: *fit_args : Positional arguments that should be passed to run_trial , for example the training and validation data. *fit_kwargs : Keyword arguments that should be passed to run_trial , for example the training and validation data.","title":"search method:"},{"location":"documentation/tuners/#set_state-method","text":"BaseTuner.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object. [source]","title":"set_state method:"},{"location":"documentation/tuners/#basetuner-class","text":"kerastuner.engine.base_tuner.BaseTuner(oracle, hypermodel, directory=None, project_name=None, logger=None, overwrite=False) Tuner base class. May be subclassed to create new tuners, including for non-Keras models. Arguments: oracle : Instance of Oracle class. hypermodel : Instance of HyperModel class (or callable that takes hyperparameters and returns a Model instance). directory : String. Path to the working directory (relative). project_name : Name to use as prefix for files saved by this Tuner. logger : Optional. Instance of Logger class, used for streaming data to Cloud Service for monitoring. overwrite : Bool, default False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project.","title":"BaseTuner class:"},{"location":"documentation/tuners/#get_best_hyperparameters-method","text":"BaseTuner.get_best_hyperparameters(num_trials=1) Returns the best hyperparameters, as determined by the objective. This method can be used to reinstantiate the (untrained) best model found during the search process. Example: best_hp = tuner.get_best_hyperparameters()[0] model = tuner.hypermodel.build(best_hp) Arguments: num_trials : (int, optional). Number of HyperParameters objects to return. HyperParameters will be returned in sorted order based on trial performance. Returns: List of HyperParameter objects.","title":"get_best_hyperparameters method:"},{"location":"documentation/tuners/#get_best_models-method_1","text":"BaseTuner.get_best_models(num_models=1) Returns the best model(s), as determined by the objective. This method is only a convenience shortcut. For best performance, It is recommended to retrain your Model on the full dataset using the best hyperparameters found during search . Arguments: num_models (int, optional). Number of best models to return. Models will be returned in sorted order. Defaults to 1. Returns: List of trained model instances.","title":"get_best_models method:"},{"location":"documentation/tuners/#get_state-method_1","text":"BaseTuner.get_state() Returns the current state of this object. This method is called during save .","title":"get_state method:"},{"location":"documentation/tuners/#load_model-method_1","text":"BaseTuner.load_model(trial) Loads a Model from a given trial. Arguments: trial : A Trial instance. For models that report intermediate results to the Oracle , generally load_model should load the best reported step by relying of trial.best_step","title":"load_model method:"},{"location":"documentation/tuners/#run_trial-method_1","text":"BaseTuner.run_trial(trial, *fit_args, **fit_kwargs) Evaluates a set of hyperparameter values. This method is called during search to evaluate a set of hyperparameters. For subclass implementers: This method is responsible for reporting metrics related to the Trial to the Oracle via self.oracle.update_trial . Simplest example: def run_trial(self, trial, x, y, val_x, val_y): model = self.hypermodel.build(trial.hyperparameters) model.fit(x, y) loss = model.evaluate(val_x, val_y) self.oracle.update_trial( trial.trial_id, {'loss': loss}) self.save_model(trial.trial_id, model) Arguments: trial : A Trial instance that contains the information needed to run this trial. Hyperparameters can be accessed via trial.hyperparameters . *fit_args : Positional arguments passed by search . *fit_kwargs : Keyword arguments passed by search .","title":"run_trial method:"},{"location":"documentation/tuners/#save_model-method_1","text":"BaseTuner.save_model(trial_id, model, step=0) Saves a Model for a given trial. Arguments: trial_id : The ID of the Trial that corresponds to this Model. model : The trained model. step : For models that report intermediate results to the Oracle , the step that this saved file should correspond to. For example, for Keras models this is the number of epochs trained.","title":"save_model method:"},{"location":"documentation/tuners/#search-method_1","text":"BaseTuner.search(*fit_args, **fit_kwargs) Performs a search for best hyperparameter configuations. Arguments: *fit_args : Positional arguments that should be passed to run_trial , for example the training and validation data. *fit_kwargs : Keyword arguments that should be passed to run_trial , for example the training and validation data.","title":"search method:"},{"location":"documentation/tuners/#set_state-method_1","text":"BaseTuner.set_state(state) Sets the current state of this object. This method is called during reload . Arguments: state: Dict. The state to restore for this object.","title":"set_state method:"},{"location":"examples/helloworld/","text":"A simple helloworld example Different workflows are shown here. from tensorflow import keras from tensorflow.keras import layers from kerastuner.tuners import RandomSearch from kerastuner.engine.hypermodel import HyperModel from kerastuner.engine.hyperparameters import HyperParameters (x, y), (val_x, val_y) = keras.datasets.mnist.load_data() x = x.astype('float32') / 255. val_x = val_x.astype('float32') / 255. x = x[:10000] y = y[:10000] \"\"\"Basic case: - We define a `build_model` function - It returns a compiled model - It uses hyperparameters defined on the fly \"\"\" def build_model(hp): model = keras.Sequential() model.add(layers.Flatten(input_shape=(28, 28))) for i in range(hp.Int('num_layers', 2, 20)): model.add(layers.Dense(units=hp.Int('units_' + str(i), 32, 512, 32), activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model tuner = RandomSearch( build_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='test_dir') tuner.search_space_summary() tuner.search(x=x, y=y, epochs=3, validation_data=(val_x, val_y)) tuner.results_summary() # \"\"\"Case #2: # - We override the loss and metrics # \"\"\" tuner = RandomSearch( build_model, objective='val_accuracy', loss=keras.losses.SparseCategoricalCrossentropy(name='my_loss'), metrics=['accuracy', 'mse'], max_trials=5, directory='test_dir') tuner.search(x, y, epochs=5, validation_data=(val_x, val_y)) # \"\"\"Case #3: # - We define a HyperModel subclass # \"\"\" class MyHyperModel(HyperModel): def __init__(self, img_size, num_classes): self.img_size = img_size self.num_classes = num_classes def build(self, hp): model = keras.Sequential() model.add(layers.Flatten(input_shape=self.img_size)) for i in range(hp.Int('num_layers', 2, 20)): model.add(layers.Dense(units=hp.Int('units_' + str(i), 32, 512, 32), activation='relu')) model.add(layers.Dense(self.num_classes, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam( hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model tuner = RandomSearch( MyHyperModel(img_size=(28, 28), num_classes=10), objective='val_accuracy', max_trials=5, directory='test_dir') tuner.search(x, y=y, epochs=5, validation_data=(val_x, val_y)) # \"\"\"Case #4: # - We restrict the search space # - This means that default values are being used for params that are left out # \"\"\" hp = HyperParameters() hp.Choice('learning_rate', [1e-1, 1e-3]) tuner = RandomSearch( build_model, max_trials=5, hyperparameters=hp, tune_new_entries=False, objective='val_accuracy') tuner.search(x=x, y=y, epochs=5, validation_data=(val_x, val_y)) # \"\"\"Case #5: # - We override specific parameters with fixed values that aren't the default # \"\"\" hp = HyperParameters() hp.Fixed('learning_rate', 0.1) tuner = RandomSearch( build_model, max_trials=5, hyperparameters=hp, tune_new_entries=True, objective='val_accuracy') tuner.search(x=x, y=y, epochs=5, validation_data=(val_x, val_y)) # \"\"\"Case #6: # - We reparameterize the search space # - This means that we override the distribution of specific hyperparameters # \"\"\" hp = HyperParameters() hp.Choice('learning_rate', [1e-1, 1e-3]) tuner = RandomSearch( build_model, max_trials=5, hyperparameters=hp, tune_new_entries=True, objective='val_accuracy') tuner.search(x=x, y=y, epochs=5, validation_data=(val_x, val_y)) # \"\"\"Case #7: # - We predefine the search space # - No unregistered parameters are allowed in `build` # \"\"\" hp = HyperParameters() hp.Choice('learning_rate', [1e-1, 1e-3]) hp.Int('num_layers', 2, 20) def build_model(hp): model = keras.Sequential() model.add(layers.Flatten(input_shape=(28, 28))) for i in range(hp.get('num_layers')): model.add(layers.Dense(32, activation='relu')) model.add(layers.Dense(10, activation='softmax')) model.compile( optimizer=keras.optimizers.Adam(hp.get('learning_rate')), loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model tuner = RandomSearch( build_model, max_trials=5, hyperparameters=hp, allow_new_entries=False, objective='val_accuracy') tuner.search(x=x, y=y, epochs=5, validation_data=(val_x, val_y))","title":"Hello World"},{"location":"tutorials/distributed-tuning/","text":"Distributed Tuning Keras Tuner makes it easy to perform distributed hyperparameter search. No changes to your code are needed to scale up from running single-threaded locally to running on dozens or hundreds of workers in parallel. Distributed Keras Tuner uses a chief-worker model. The chief runs a service to which the workers report results and query for the hyperparameters to try next. The chief should be run on a single-threaded CPU instance (or alternatively as a separate process on one of the workers). Configuring distributed mode Configuring distributed mode for Keras Tuner only requires setting three environment variables: KERASTUNER_TUNER_ID : This should be set to \"chief\" for the chief process. Other workers should be passed a unique ID (by convention, \"tuner0\", \"tuner1\", etc). KERASTUNER_ORACLE_IP : The IP address or hostname that the chief service should run on. All workers should be able to resolve and access this address. KERASTUNER_ORACLE_PORT : The port that the chief service should run on. This can be freely chosen, but must be a port that is accessible to the other workers. Instances communicate via the gRPC protocol. The same code can be run on all workers. Additional considerations for distributed mode are: All workers should have access to a centralized file system to which they can write their results. All workers should be able to access the necessary training and validation data needed for tuning. To support fault-tolerance, overwrite should be kept as False in Tuner.__init__ ( False is the default). Example bash script for chief service (sample code for run_tuning.py at bottom of page): export KERASTUNER_TUNER_ID=\"chief\" export KERASTUNER_ORACLE_IP=\"127.0.0.1\" export KERASTUNER_ORACLE_PORT=\"8000\" python run_tuning.py Example bash script for worker: export KERASTUNER_TUNER_ID=\"tuner0\" export KERASTUNER_ORACLE_IP=\"127.0.0.1\" export KERASTUNER_ORACLE_PORT=\"8000\" python run_tuning.py Data parallelism with tf.distribute Keras Tuner also supports data parallelism via tf.distribute . Data parallelism and distributed tuning can be combined. For example, if you have 10 workers with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs by using tf.distribute.MirroredStrategy . You can also run each trial on TPUs via tf.distribute.experimental.TPUStrategy . Currently tf.distribute.MultiWorkerMirroredStrategy is not supported, but support for this is on the roadmap. Example code When the enviroment variables described above are set, the example below will run distributed tuning and will also use data parallelism within each trial via tf.distribute . The example loads MNIST from tensorflow_datasets and uses hyperband for the hyperparameter search. import kerastuner as kt import tensorflow as tf import tensorflow_datasets as tfds def build_model(hp): \"\"\"Builds a convolutional model.\"\"\" inputs = tf.keras.Input(shape=(28, 28, 1)) x = inputs for i in range(hp.Int('conv_layers', 1, 3, default=3)): x = tf.keras.layers.Conv2D( filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8), kernel_size=hp.Int('kernel_size_' + str(i), 3, 5), activation='relu', padding='same')(x) if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max': x = tf.keras.layers.MaxPooling2D()(x) else: x = tf.keras.layers.AveragePooling2D()(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.ReLU()(x) if hp.Choice('global_pooling', ['max', 'avg']) == 'max': x = tf.keras.layers.GlobalMaxPooling2D()(x) else: x = tf.keras.layers.GlobalAveragePooling2D()(x) outputs = tf.keras.layers.Dense(10, activation='softmax')(x) model = tf.keras.Model(inputs, outputs) optimizer = hp.Choice('optimizer', ['adam', 'sgd']) model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model def convert_dataset(item): \"\"\"Puts the mnist dataset in the format Keras expects, (features, labels).\"\"\" image = item['image'] label = item['label'] image = tf.dtypes.cast(image, 'float32') / 255. return image, label def main(): \"\"\"Runs the hyperparameter search.\"\"\" tuner = kt.Hyperband( hypermodel=build_model, objective='val_accuracy', max_epochs=8, factor=2, hyperband_iterations=3, distribution_strategy=tf.distribute.MirroredStrategy(), directory='results_dir', project_name='mnist') mnist_data = tfds.load('mnist') mnist_train, mnist_test = mnist_data['train'], mnist_data['test'] mnist_train = mnist_train.map(convert_dataset).shuffle(1000).batch(100).repeat() mnist_test = mnist_test.map(convert_dataset).batch(100) tuner.search(mnist_train, steps_per_epoch=600, validation_data=mnist_test, validation_steps=100, epochs=20, callbacks=[tf.keras.callbacks.EarlyStopping('val_accuracy')]) if __name__ == '__main__': main()","title":"Distributed Tuning"},{"location":"tutorials/distributed-tuning/#distributed-tuning","text":"Keras Tuner makes it easy to perform distributed hyperparameter search. No changes to your code are needed to scale up from running single-threaded locally to running on dozens or hundreds of workers in parallel. Distributed Keras Tuner uses a chief-worker model. The chief runs a service to which the workers report results and query for the hyperparameters to try next. The chief should be run on a single-threaded CPU instance (or alternatively as a separate process on one of the workers).","title":"Distributed Tuning"},{"location":"tutorials/distributed-tuning/#configuring-distributed-mode","text":"Configuring distributed mode for Keras Tuner only requires setting three environment variables: KERASTUNER_TUNER_ID : This should be set to \"chief\" for the chief process. Other workers should be passed a unique ID (by convention, \"tuner0\", \"tuner1\", etc). KERASTUNER_ORACLE_IP : The IP address or hostname that the chief service should run on. All workers should be able to resolve and access this address. KERASTUNER_ORACLE_PORT : The port that the chief service should run on. This can be freely chosen, but must be a port that is accessible to the other workers. Instances communicate via the gRPC protocol. The same code can be run on all workers. Additional considerations for distributed mode are: All workers should have access to a centralized file system to which they can write their results. All workers should be able to access the necessary training and validation data needed for tuning. To support fault-tolerance, overwrite should be kept as False in Tuner.__init__ ( False is the default). Example bash script for chief service (sample code for run_tuning.py at bottom of page): export KERASTUNER_TUNER_ID=\"chief\" export KERASTUNER_ORACLE_IP=\"127.0.0.1\" export KERASTUNER_ORACLE_PORT=\"8000\" python run_tuning.py Example bash script for worker: export KERASTUNER_TUNER_ID=\"tuner0\" export KERASTUNER_ORACLE_IP=\"127.0.0.1\" export KERASTUNER_ORACLE_PORT=\"8000\" python run_tuning.py","title":"Configuring distributed mode"},{"location":"tutorials/distributed-tuning/#data-parallelism-with-tfdistribute","text":"Keras Tuner also supports data parallelism via tf.distribute . Data parallelism and distributed tuning can be combined. For example, if you have 10 workers with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs by using tf.distribute.MirroredStrategy . You can also run each trial on TPUs via tf.distribute.experimental.TPUStrategy . Currently tf.distribute.MultiWorkerMirroredStrategy is not supported, but support for this is on the roadmap.","title":"Data parallelism with tf.distribute"},{"location":"tutorials/distributed-tuning/#example-code","text":"When the enviroment variables described above are set, the example below will run distributed tuning and will also use data parallelism within each trial via tf.distribute . The example loads MNIST from tensorflow_datasets and uses hyperband for the hyperparameter search. import kerastuner as kt import tensorflow as tf import tensorflow_datasets as tfds def build_model(hp): \"\"\"Builds a convolutional model.\"\"\" inputs = tf.keras.Input(shape=(28, 28, 1)) x = inputs for i in range(hp.Int('conv_layers', 1, 3, default=3)): x = tf.keras.layers.Conv2D( filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8), kernel_size=hp.Int('kernel_size_' + str(i), 3, 5), activation='relu', padding='same')(x) if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max': x = tf.keras.layers.MaxPooling2D()(x) else: x = tf.keras.layers.AveragePooling2D()(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.ReLU()(x) if hp.Choice('global_pooling', ['max', 'avg']) == 'max': x = tf.keras.layers.GlobalMaxPooling2D()(x) else: x = tf.keras.layers.GlobalAveragePooling2D()(x) outputs = tf.keras.layers.Dense(10, activation='softmax')(x) model = tf.keras.Model(inputs, outputs) optimizer = hp.Choice('optimizer', ['adam', 'sgd']) model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model def convert_dataset(item): \"\"\"Puts the mnist dataset in the format Keras expects, (features, labels).\"\"\" image = item['image'] label = item['label'] image = tf.dtypes.cast(image, 'float32') / 255. return image, label def main(): \"\"\"Runs the hyperparameter search.\"\"\" tuner = kt.Hyperband( hypermodel=build_model, objective='val_accuracy', max_epochs=8, factor=2, hyperband_iterations=3, distribution_strategy=tf.distribute.MirroredStrategy(), directory='results_dir', project_name='mnist') mnist_data = tfds.load('mnist') mnist_train, mnist_test = mnist_data['train'], mnist_data['test'] mnist_train = mnist_train.map(convert_dataset).shuffle(1000).batch(100).repeat() mnist_test = mnist_test.map(convert_dataset).batch(100) tuner.search(mnist_train, steps_per_epoch=600, validation_data=mnist_test, validation_steps=100, epochs=20, callbacks=[tf.keras.callbacks.EarlyStopping('val_accuracy')]) if __name__ == '__main__': main()","title":"Example code"},{"location":"tutorials/subclass-tuner/","text":"Subclassing Tuner for Custom Training Loops The Tuner class at kerastuner.engine.tuner.Tuner can be subclassed to support advanced uses such as: Custom training loops (GANs, reinforement learning, etc.) Adding hyperparameters outside of the model builing function (preprocessing, data augmentation, test time augmentation, etc.) This tutorial will not cover subclassing to support non-Keras models. To accomplish this, you can subclass the kerastuner.engine.base_tuner.BaseTuner class (See kerastuner.tuners.sklearn.Sklearn for an example). Understanding the search process. Tuner.search can be passed any arguments. These arguments will be passed directly to Tuner.run_trial , along with a Trial object that contains information about the current trial, including hyperparameters and the status of the trial. Typically, Tuner.run_trial is the only method that users need to override when subclassing Tuner . Overriding run_trial . There are two ways to write run_trial . One is to leverage Tuner 's built-in callback hooks, which send the value of the objective to the Oracle and save the latest state of the Model. These hooks are: self.on_epoch_end : Must be called. Reports results to the Oracle and saves the Model. The logs dictionary passed to this method must contain the objective name. self.on_epoch_begin , self.on_batch_begin , self.on_batch_end : Optional. These methods do nothing in Tuner , but are useful to provide as hooks if you expect users of your subclass to create their own subclasses that override these parts of the training process. class MyTuner(kt.Tuner): def run_trial(self, trial, ...): model = self.hypermodel.build(trial.hyperparameters) for epoch in range(10): epoch_loss = ... self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss}) Alternatively, you can instead directly call the methods used to report results to the Oracle and save the Model. This can allow more flexibility for use cases where there is no natural concept of epoch or where you do not want to report results to the Oracle after each epoch. These methods are: self.oracle.update_trial : Reports current results to the Oracle . The metrics dictionary passed to this method must contain the objective name. self.save_model : Saves the trained model. class MyTuner(kt.Tuner): def run_trial(self, trial, ...): model = self.hypermodel.build(trial.hyperparameters) score = ... self.oracle.update_trial(trial.trial_id, {'score': score}) self.oracle.save_model(trail.trial_id, model) Adding HyperParameters during preprocessing, evaluation, etc. New HyperParameter s can be defined anywhere in run_trial , in the same way that HyperParameter s are defined in a HyperModel . These hyperparameters take on their default value the first time they are encountered, and thereafter are tuned by the Oracle . class MyTuner(kt.Tuner): def run_trial(self, trial, ...): hp = trial.hyperparameters model = self.hypermodel.build(hp) batch_size = hp.Int('batch_size', 32, 128, step=32) random_flip = hp.Boolean('random_flip') ... End-to-end Example: import kerastuner as kt import tensorflow as tf import tensorflow_datasets as tfds def build_model(hp): \"\"\"Builds a convolutional model.\"\"\" inputs = tf.keras.Input(shape=(28, 28, 1)) x = inputs for i in range(hp.Int('conv_layers', 1, 3, default=3)): x = tf.keras.layers.Conv2D( filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8), kernel_size=hp.Int('kernel_size_' + str(i), 3, 5), activation='relu', padding='same')(x) if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max': x = tf.keras.layers.MaxPooling2D()(x) else: x = tf.keras.layers.AveragePooling2D()(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.ReLU()(x) if hp.Choice('global_pooling', ['max', 'avg']) == 'max': x = tf.keras.layers.GlobalMaxPooling2D()(x) else: x = tf.keras.layers.GlobalAveragePooling2D()(x) outputs = tf.keras.layers.Dense(10, activation='softmax')(x) model = tf.keras.Model(inputs, outputs) optimizer = hp.Choice('optimizer', ['adam', 'sgd']) model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model class MyTuner(kt.Tuner): def run_trial(self, trial, train_ds): hp = trial.hyperparameters # Hyperparameters can be added anywhere inside `run_trial`. # When the first trial is run, they will take on their default values. # Afterwards, they will be tuned by the `Oracle`. train_ds = train_ds.batch( hp.Int('batch_size', 32, 128, step=32, default=64)) model = self.hypermodel.build(trial.hyperparameters) lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3) optimizer = tf.keras.optimizers.Adam(lr) epoch_loss_metric = tf.keras.metrics.Mean() @tf.function def run_train_step(data): images = tf.dtypes.cast(data['image'], 'float32') / 255. labels = data['label'] with tf.GradientTape() as tape: logits = model(images) loss = tf.keras.losses.sparse_categorical_crossentropy( labels, logits) # Add any regularization losses. if model.losses: loss += tf.math.add_n(model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) epoch_loss_metric.update_state(loss) return loss # `self.on_epoch_end` reports results to the `Oracle` and saves the # current state of the Model. The other hooks called here only log values # for display but can also be overridden. For use cases where there is no # natural concept of epoch, you do not have to call any of these hooks. In # this case you should instead call `self.oracle.update_trial` and # `self.oracle.save_model` manually. for epoch in range(10): print('Epoch: {}'.format(epoch)) self.on_epoch_begin(trial, model, epoch, logs={}) for batch, data in enumerate(train_ds): self.on_batch_begin(trial, model, batch, logs={}) batch_loss = float(run_train_step(data)) self.on_batch_end(trial, model, batch, logs={'loss': batch_loss}) if batch % 100 == 0: loss = epoch_loss_metric.result().numpy() print('Batch: {}, Average Loss: {}'.format(batch, loss)) epoch_loss = epoch_loss_metric.result().numpy() self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss}) epoch_loss_metric.reset_states() def main(): tuner = MyTuner( oracle=kt.oracles.BayesianOptimization( objective=kt.Objective('loss', 'min'), max_trials=2), hypermodel=build_model, directory='results', project_name='mnist_custom_training') mnist_data = tfds.load('mnist') mnist_train, mnist_test = mnist_data['train'], mnist_data['test'] mnist_train = mnist_train.shuffle(1000) tuner.search(train_ds=mnist_train) best_hps = tuner.get_best_hyperparameters()[0] print(best_hps.values) best_model = tuner.get_best_models()[0] if __name__ == '__main__': main()","title":"Subclassing Tuner for Custom Training Loops"},{"location":"tutorials/subclass-tuner/#subclassing-tuner-for-custom-training-loops","text":"The Tuner class at kerastuner.engine.tuner.Tuner can be subclassed to support advanced uses such as: Custom training loops (GANs, reinforement learning, etc.) Adding hyperparameters outside of the model builing function (preprocessing, data augmentation, test time augmentation, etc.) This tutorial will not cover subclassing to support non-Keras models. To accomplish this, you can subclass the kerastuner.engine.base_tuner.BaseTuner class (See kerastuner.tuners.sklearn.Sklearn for an example).","title":"Subclassing Tuner for Custom Training Loops"},{"location":"tutorials/subclass-tuner/#understanding-the-search-process","text":"Tuner.search can be passed any arguments. These arguments will be passed directly to Tuner.run_trial , along with a Trial object that contains information about the current trial, including hyperparameters and the status of the trial. Typically, Tuner.run_trial is the only method that users need to override when subclassing Tuner .","title":"Understanding the search process."},{"location":"tutorials/subclass-tuner/#overriding-run_trial","text":"There are two ways to write run_trial . One is to leverage Tuner 's built-in callback hooks, which send the value of the objective to the Oracle and save the latest state of the Model. These hooks are: self.on_epoch_end : Must be called. Reports results to the Oracle and saves the Model. The logs dictionary passed to this method must contain the objective name. self.on_epoch_begin , self.on_batch_begin , self.on_batch_end : Optional. These methods do nothing in Tuner , but are useful to provide as hooks if you expect users of your subclass to create their own subclasses that override these parts of the training process. class MyTuner(kt.Tuner): def run_trial(self, trial, ...): model = self.hypermodel.build(trial.hyperparameters) for epoch in range(10): epoch_loss = ... self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss}) Alternatively, you can instead directly call the methods used to report results to the Oracle and save the Model. This can allow more flexibility for use cases where there is no natural concept of epoch or where you do not want to report results to the Oracle after each epoch. These methods are: self.oracle.update_trial : Reports current results to the Oracle . The metrics dictionary passed to this method must contain the objective name. self.save_model : Saves the trained model. class MyTuner(kt.Tuner): def run_trial(self, trial, ...): model = self.hypermodel.build(trial.hyperparameters) score = ... self.oracle.update_trial(trial.trial_id, {'score': score}) self.oracle.save_model(trail.trial_id, model)","title":"Overriding run_trial."},{"location":"tutorials/subclass-tuner/#adding-hyperparameters-during-preprocessing-evaluation-etc","text":"New HyperParameter s can be defined anywhere in run_trial , in the same way that HyperParameter s are defined in a HyperModel . These hyperparameters take on their default value the first time they are encountered, and thereafter are tuned by the Oracle . class MyTuner(kt.Tuner): def run_trial(self, trial, ...): hp = trial.hyperparameters model = self.hypermodel.build(hp) batch_size = hp.Int('batch_size', 32, 128, step=32) random_flip = hp.Boolean('random_flip') ...","title":"Adding HyperParameters during preprocessing, evaluation, etc."},{"location":"tutorials/subclass-tuner/#end-to-end-example","text":"import kerastuner as kt import tensorflow as tf import tensorflow_datasets as tfds def build_model(hp): \"\"\"Builds a convolutional model.\"\"\" inputs = tf.keras.Input(shape=(28, 28, 1)) x = inputs for i in range(hp.Int('conv_layers', 1, 3, default=3)): x = tf.keras.layers.Conv2D( filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8), kernel_size=hp.Int('kernel_size_' + str(i), 3, 5), activation='relu', padding='same')(x) if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max': x = tf.keras.layers.MaxPooling2D()(x) else: x = tf.keras.layers.AveragePooling2D()(x) x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.ReLU()(x) if hp.Choice('global_pooling', ['max', 'avg']) == 'max': x = tf.keras.layers.GlobalMaxPooling2D()(x) else: x = tf.keras.layers.GlobalAveragePooling2D()(x) outputs = tf.keras.layers.Dense(10, activation='softmax')(x) model = tf.keras.Model(inputs, outputs) optimizer = hp.Choice('optimizer', ['adam', 'sgd']) model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) return model class MyTuner(kt.Tuner): def run_trial(self, trial, train_ds): hp = trial.hyperparameters # Hyperparameters can be added anywhere inside `run_trial`. # When the first trial is run, they will take on their default values. # Afterwards, they will be tuned by the `Oracle`. train_ds = train_ds.batch( hp.Int('batch_size', 32, 128, step=32, default=64)) model = self.hypermodel.build(trial.hyperparameters) lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3) optimizer = tf.keras.optimizers.Adam(lr) epoch_loss_metric = tf.keras.metrics.Mean() @tf.function def run_train_step(data): images = tf.dtypes.cast(data['image'], 'float32') / 255. labels = data['label'] with tf.GradientTape() as tape: logits = model(images) loss = tf.keras.losses.sparse_categorical_crossentropy( labels, logits) # Add any regularization losses. if model.losses: loss += tf.math.add_n(model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) epoch_loss_metric.update_state(loss) return loss # `self.on_epoch_end` reports results to the `Oracle` and saves the # current state of the Model. The other hooks called here only log values # for display but can also be overridden. For use cases where there is no # natural concept of epoch, you do not have to call any of these hooks. In # this case you should instead call `self.oracle.update_trial` and # `self.oracle.save_model` manually. for epoch in range(10): print('Epoch: {}'.format(epoch)) self.on_epoch_begin(trial, model, epoch, logs={}) for batch, data in enumerate(train_ds): self.on_batch_begin(trial, model, batch, logs={}) batch_loss = float(run_train_step(data)) self.on_batch_end(trial, model, batch, logs={'loss': batch_loss}) if batch % 100 == 0: loss = epoch_loss_metric.result().numpy() print('Batch: {}, Average Loss: {}'.format(batch, loss)) epoch_loss = epoch_loss_metric.result().numpy() self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss}) epoch_loss_metric.reset_states() def main(): tuner = MyTuner( oracle=kt.oracles.BayesianOptimization( objective=kt.Objective('loss', 'min'), max_trials=2), hypermodel=build_model, directory='results', project_name='mnist_custom_training') mnist_data = tfds.load('mnist') mnist_train, mnist_test = mnist_data['train'], mnist_data['test'] mnist_train = mnist_train.shuffle(1000) tuner.search(train_ds=mnist_train) best_hps = tuner.get_best_hyperparameters()[0] print(best_hps.values) best_model = tuner.get_best_models()[0] if __name__ == '__main__': main()","title":"End-to-end Example:"}]}